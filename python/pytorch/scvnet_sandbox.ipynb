{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clf_funcs import fit, get_mnist_loaders, SimpleConvNet\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 96\n",
    "test_batch_size = 128\n",
    "epochs = 3\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "log_interval = 300\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvNet()\n",
    "model = model.to(device)\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "loss_func = F.nll_loss\n",
    "\n",
    "train_dl, _, test_dl = get_mnist_loaders(batch_size, test_batch_size, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\t[299/625 (48%)]\tLoss 0.9373\n",
      "[1]\t[599/625 (96%)]\tLoss 0.1225\n",
      "[2]\t[299/625 (48%)]\tLoss 0.0760\n",
      "[2]\t[599/625 (96%)]\tLoss 0.0658\n",
      "[3]\t[299/625 (48%)]\tLoss 0.0489\n",
      "[3]\t[599/625 (96%)]\tLoss 0.0450\n",
      "times without CUDA event sync:  [6.8949208757, 6.6692102669, 6.6709405586]\n"
     ]
    }
   ],
   "source": [
    "time_elapsed = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\tstart = time.time_ns()\n",
    "\ttrain_history = fit(model, device, train_dl, loss_func, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\tend = time.time_ns()\n",
    "\n",
    "\ttime_elapsed.append((end - start) / 10e9)\n",
    "\n",
    "print('times without CUDA event sync: ', time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\t[299/625 (48%)]\tLoss 0.0165\n",
      "[1]\t[599/625 (96%)]\tLoss 0.0175\n",
      "[2]\t[299/625 (48%)]\tLoss 0.0144\n",
      "[2]\t[599/625 (96%)]\tLoss 0.0152\n",
      "[3]\t[299/625 (48%)]\tLoss 0.0121\n",
      "[3]\t[599/625 (96%)]\tLoss 0.0123\n",
      "times with CUDA event sync:  [66461.9140625, 66703.78125, 66742.078125]\n"
     ]
    }
   ],
   "source": [
    "time_elapsed = []\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\tstart.record()\n",
    "\ttrain_history = fit(model, device, train_dl, loss_func, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\tend.record()\n",
    "\ttorch.cuda.synchronize()\n",
    "\n",
    "\ttime_elapsed.append(start.elapsed_time(end))\n",
    "\n",
    "print('times with CUDA event sync: ', time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for whatever reason using cuda events for profiling makes the training take significantly more time than it should for this one model only?? turning sync off results in the following error on `Event::elapsed_time`  \n",
    "\n",
    "```\n",
    "---> 11 \ttime_elapsed.append(start.elapsed_time(end))\n",
    "...\n",
    "RuntimeError: CUDA error: device not ready\n",
    "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
    "```\n",
    "\n",
    "Event sync also makes DCGAN borderline impossible (as shown in some other notebook idc) and generally makes training loops behave unpredictable. Considering refactoring pytorch code to use `time::time_ns`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
