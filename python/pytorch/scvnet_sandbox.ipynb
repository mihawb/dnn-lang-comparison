{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from load_datasets import load_mnist_imgs_and_labels\n",
    "from clf_funcs import fit, get_mnist_loaders, SimpleConvNet\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 96\n",
    "test_batch_size = 128\n",
    "epochs = 3\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "log_interval = 300\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telemetry_fit(model, device, train_dl, opt) -> list:\n",
    "\ttime_elapsed = []\n",
    "\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\tprint(f'epoch {epoch} start: {datetime.datetime.fromtimestamp(time.time()).strftime(\"%H:%M:%S\")}')\n",
    "\t\t\n",
    "\t\tstart = time.perf_counter_ns()\n",
    "\t\ttrain_history = fit(model, device, train_dl, F.nll_loss, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\t\tend = time.perf_counter_ns()\n",
    "\n",
    "\t\tprint(f'epoch {epoch} end: {datetime.datetime.fromtimestamp(time.time()).strftime(\"%H:%M:%S\")}')\t\n",
    "\t\tprint(f'seconds elapsed: {(end - start) / 1e9}')\n",
    "\n",
    "\t\ttime_elapsed.append((end - start))\n",
    "\n",
    "\treturn time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 start: 19:31:40\n",
      "[1]\t[299/625 (48%)]\tLoss 0.7746\n",
      "[1]\t[599/625 (96%)]\tLoss 0.1234\n",
      "epoch 1 end: 19:32:47\n",
      "seconds elapsed: 66.685224503\n",
      "epoch 2 start: 19:32:47\n",
      "[2]\t[299/625 (48%)]\tLoss 0.0849\n",
      "[2]\t[599/625 (96%)]\tLoss 0.0680\n",
      "epoch 2 end: 19:33:53\n",
      "seconds elapsed: 66.033280051\n",
      "epoch 3 start: 19:33:53\n",
      "[3]\t[299/625 (48%)]\tLoss 0.0553\n",
      "[3]\t[599/625 (96%)]\tLoss 0.0498\n",
      "epoch 3 end: 19:34:59\n",
      "seconds elapsed: 66.163545291\n",
      "times without CUDA event sync:  [66685224503, 66033280051, 66163545291]\n"
     ]
    }
   ],
   "source": [
    "model = SimpleConvNet()\n",
    "model = model.to(device)\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "train_dl, _, test_dl = get_mnist_loaders(batch_size, test_batch_size, flatten=False)\n",
    "\n",
    "time_elapsed = telemetry_fit(model, device, train_dl, opt)\n",
    "\n",
    "print('times without CUDA event sync: ', time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 start: 19:34:59\n",
      "[1]\t[299/625 (48%)]\tLoss 0.0372\n",
      "[1]\t[599/625 (96%)]\tLoss 0.0423\n",
      "epoch 1 end: 19:36:05\n",
      "seconds elapsed: 66.1485\n",
      "epoch 2 start: 19:36:05\n",
      "[2]\t[299/625 (48%)]\tLoss 0.0306\n",
      "[2]\t[599/625 (96%)]\tLoss 0.0319\n",
      "epoch 2 end: 19:37:11\n",
      "seconds elapsed: 66.1470859375\n",
      "epoch 3 start: 19:37:11\n",
      "[3]\t[299/625 (48%)]\tLoss 0.0250\n",
      "[3]\t[599/625 (96%)]\tLoss 0.0264\n",
      "epoch 3 end: 19:38:18\n",
      "seconds elapsed: 66.12184375\n",
      "times with CUDA event sync:  [66148.5, 66147.0859375, 66121.84375]\n"
     ]
    }
   ],
   "source": [
    "time_elapsed_sync = []\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\tprint(f'epoch {epoch} start: {datetime.datetime.fromtimestamp(time.time()).strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "\tstart.record()\n",
    "\ttrain_history = fit(model, device, train_dl, F.nll_loss, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\tend.record()\n",
    "\ttorch.cuda.synchronize()\n",
    "\n",
    "\tprint(f'epoch {epoch} end: {datetime.datetime.fromtimestamp(time.time()).strftime(\"%H:%M:%S\")}')\t\n",
    "\tprint(f'seconds elapsed: {start.elapsed_time(end) / 1e3}')\n",
    "\n",
    "\ttime_elapsed_sync.append(start.elapsed_time(end))\n",
    "\n",
    "print('times with CUDA event sync: ', time_elapsed_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scvnet in pytorch trains about 10 times slower than the same model on the same dataset in tensorflow or even libtorch which is surprising. my guess was some issues with cuda events but apparently thats not it. turning sync off results in the following error on `Event::elapsed_time`  \n",
    "\n",
    "```\n",
    "---> 16 \ttime_elapsed_sync.append(start.elapsed_time(end))\n",
    "...\n",
    "RuntimeError: CUDA error: device not ready\n",
    "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
    "```\n",
    "\n",
    "However, event sync still makes DCGAN borderline impossible (as shown in some other notebook idc) and generally makes training loops behave unpredictable. Considering refactoring pytorch code to use `time::perf_counter_ns`\n",
    "\n",
    "Next best guess would be lazy dataset loading, but that is also not the case since MNIST dataset is being read eagerly from binary files into tensors in memory before dataloader creation. Below `list[tuple[Tensor, Tensor]]` is used to create `DataLoader` (in lieu of `TensorDataset`) but it obviously changes nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 start: 20:21:16\n",
      "[1]\t[299/625 (48%)]\tLoss 0.0373\n",
      "[1]\t[599/625 (96%)]\tLoss 0.0388\n",
      "epoch 1 end: 20:22:23\n",
      "seconds elapsed: 66.268160705\n",
      "epoch 2 start: 20:22:23\n",
      "[2]\t[299/625 (48%)]\tLoss 0.0302\n",
      "[2]\t[599/625 (96%)]\tLoss 0.0301\n",
      "epoch 2 end: 20:23:29\n",
      "seconds elapsed: 66.41212843\n",
      "epoch 3 start: 20:23:29\n",
      "[3]\t[299/625 (48%)]\tLoss 0.0266\n",
      "[3]\t[599/625 (96%)]\tLoss 0.0236\n",
      "epoch 3 end: 20:24:36\n",
      "seconds elapsed: 66.605874152\n",
      "times with list based dataset:  [66268160705, 66412128430, 66605874152]\n"
     ]
    }
   ],
   "source": [
    "time_elapsed_listds = []\n",
    "train_dl_list, _, _ = get_mnist_loaders(batch_size, test_batch_size, flatten=False, pt_ds=False)\n",
    "\n",
    "time_elapsed_listds = telemetry_fit(model, device, train_dl_list, opt)\n",
    "\n",
    "print('times with list based dataset: ', time_elapsed_listds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about completely omitting `torch.DataLoader` and passing in a list of dataclasses? Maybe the DL does some memory shenanigans im not aware of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 start: 20:42:09\n",
      "[1]\t[299/625 (48%)]\tLoss 0.0205\n",
      "[1]\t[599/625 (96%)]\tLoss 0.0230\n",
      "epoch 1 end: 20:43:15\n",
      "seconds elapsed: 65.997850772\n",
      "epoch 2 start: 20:43:15\n",
      "[2]\t[299/625 (48%)]\tLoss 0.0169\n",
      "[2]\t[599/625 (96%)]\tLoss 0.0177\n",
      "epoch 2 end: 20:44:22\n",
      "seconds elapsed: 66.169891201\n",
      "epoch 3 start: 20:44:22\n",
      "[3]\t[299/625 (48%)]\tLoss 0.0137\n",
      "[3]\t[599/625 (96%)]\tLoss 0.0145\n",
      "epoch 3 end: 20:45:28\n",
      "seconds elapsed: 66.415712678\n",
      "times with list based dataset:  [65997850772, 66169891201, 66415712678]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_mnist_imgs_and_labels(\n",
    "\t'../../datasets/mnist-digits/train-images-idx3-ubyte',\n",
    "\t'../../datasets/mnist-digits/train-labels-idx1-ubyte'\n",
    ")\n",
    "\n",
    "x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "\n",
    "x_train, y_train = map(\n",
    "\tlambda x: torch.tensor(x).split(batch_size, 0),\n",
    "\t(x_train, y_train)\n",
    ")\n",
    "\n",
    "tuple_loader = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "len(tuple_loader), tuple_loader[0][0].shape, type(tuple_loader[0][0])\n",
    "\n",
    "time_elapsed_tupledl = telemetry_fit(model, device, tuple_loader, opt)\n",
    "\n",
    "print('times with list based dataset: ', time_elapsed_tupledl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mkay so its not loading data, now lets check model architecture (although the internet knows nothing about conv2d in pytorch having worse performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSCVNet(nn.Module):\n",
    "\n",
    "\tdef __init__(self, num_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(1, 16, 5, 1, 2), # no dtype map\n",
    "\t\t\tnn.ReLU(),    \n",
    "\t\t\tnn.MaxPool2d(2)\n",
    "\t\t)\n",
    "\t\tself.conv2 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(16, 32, 5, 1, 2), # no dtype map\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\t\t)\n",
    "\t\tself.dense = nn.Linear(32 * 7 * 7, 500) # no dtype map\n",
    "\t\tself.classifier = nn.Linear(500, num_classes) # no dtype map\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.dense(x))\n",
    "\t\treturn F.log_softmax(self.classifier(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 start: 21:22:55\n",
      "[1]\t[299/625 (48%)]\tLoss 2.3056\n",
      "[1]\t[599/625 (96%)]\tLoss 2.3053\n",
      "epoch 1 end: 21:23:00\n",
      "seconds elapsed: 5.401223702\n",
      "epoch 2 start: 21:23:00\n",
      "[2]\t[299/625 (48%)]\tLoss 2.3056\n",
      "[2]\t[599/625 (96%)]\tLoss 2.3053\n",
      "epoch 2 end: 21:23:03\n",
      "seconds elapsed: 3.334495395\n",
      "epoch 3 start: 21:23:03\n",
      "[3]\t[299/625 (48%)]\tLoss 2.3056\n",
      "[3]\t[599/625 (96%)]\tLoss 2.3053\n",
      "epoch 3 end: 21:23:07\n",
      "seconds elapsed: 3.270358371\n",
      "times without dtype mapping:  [5401223702, 3334495395, 3270358371]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_mnist_imgs_and_labels(\n",
    "\t'../../datasets/mnist-digits/train-images-idx3-ubyte',\n",
    "\t'../../datasets/mnist-digits/train-labels-idx1-ubyte'\n",
    ")\n",
    "\n",
    "x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "\n",
    "x_train, y_train = map(\n",
    "\tlambda x: torch.tensor(x).split(batch_size, 0),\n",
    "\t(x_train, y_train)\n",
    ")\n",
    "x_train = map(lambda x: x.to(torch.float32), x_train) # np.float64 to torch.float32\n",
    "\n",
    "tuple_loader = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "\n",
    "model = NewSCVNet()\n",
    "model = model.to(device)\n",
    "\n",
    "time_elapsed_dtype_f32 = telemetry_fit(model, device, tuple_loader, opt)\n",
    "\n",
    "print('times without dtype mapping: ', time_elapsed_dtype_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for fucks sake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
