{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2, resnet50\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from clf_funcs import fit, test, get_cifar10_loaders, get_mnist_loaders, FullyConnectedNet, SimpleConvNet\n",
    "from dcgan_funcs import fit_dcgan, get_celeba_loader, Discriminator, Generator, dcgan_weights_init, generate\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "epochs = 2\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "log_interval = 300\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenet_v2()\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mobilenet_v2()\n",
    "# model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "train_dl, test_dl = get_cifar10_loaders(batch_size, test_batch_size)\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "\ttrain_history = fit(model, device, train_dl, loss_func, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\t_, accuracy = test(model, device, test_dl, loss_func, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 3\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "lr = 1e-2\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(nc, nz, ngf).to(device)\n",
    "netD = Discriminator(nc, ndf).to(device)\n",
    "netG.apply(dcgan_weights_init)\n",
    "netD.apply(dcgan_weights_init)\n",
    "\n",
    "celeba_dl = get_celeba_loader(batch_size=batch_size, root='../../datasets/celeba_trunc/')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1.\n",
    "fake_label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1][49/391]\tLoss_G: 9.9866\tLoss_D: 0.1113\tD(x): 0.9780\tD(G(z)): 0.0023 / 0.0018\n",
      "[1][99/391]\tLoss_G: 5.2297\tLoss_D: 4.4584\tD(x): 0.4519\tD(G(z)): 0.2775 / 0.2941\n",
      "[1][149/391]\tLoss_G: 4.2810\tLoss_D: 1.0680\tD(x): 0.7384\tD(G(z)): 0.1684 / 0.1126\n",
      "[1][199/391]\tLoss_G: 3.1357\tLoss_D: 2.0294\tD(x): 0.5194\tD(G(z)): 0.2952 / 0.2186\n",
      "[1][249/391]\tLoss_G: 2.0193\tLoss_D: 1.1808\tD(x): 0.5634\tD(G(z)): 0.2810 / 0.2469\n",
      "[1][299/391]\tLoss_G: 1.2344\tLoss_D: 1.1710\tD(x): 0.5304\tD(G(z)): 0.3170 / 0.3293\n",
      "[1][349/391]\tLoss_G: 2.0622\tLoss_D: 0.9743\tD(x): 0.6035\tD(G(z)): 0.2894 / 0.1775\n",
      "{'loss_G': 6.082088980163847, 'loss_D': 3.3189082296232555, 'D_x': 0.6962612921337079, 'D_G_z1': 0.32553051516967396, 'D_G_z2': 0.1598115035920408}\n",
      "[2][49/391]\tLoss_G: 2.0631\tLoss_D: 0.7526\tD(x): 0.7955\tD(G(z)): 0.3376 / 0.1626\n",
      "[2][99/391]\tLoss_G: 1.2409\tLoss_D: 1.3677\tD(x): 0.4339\tD(G(z)): 0.2930 / 0.3419\n",
      "[2][149/391]\tLoss_G: 1.3118\tLoss_D: 1.1015\tD(x): 0.4495\tD(G(z)): 0.1795 / 0.3014\n",
      "[2][199/391]\tLoss_G: 2.8279\tLoss_D: 1.4127\tD(x): 0.8475\tD(G(z)): 0.6181 / 0.0905\n",
      "[2][249/391]\tLoss_G: 2.1089\tLoss_D: 1.3531\tD(x): 0.6577\tD(G(z)): 0.5394 / 0.1472\n",
      "[2][299/391]\tLoss_G: 1.5832\tLoss_D: 0.9509\tD(x): 0.5860\tD(G(z)): 0.2801 / 0.2280\n",
      "[2][349/391]\tLoss_G: 1.3283\tLoss_D: 1.1703\tD(x): 0.4211\tD(G(z)): 0.1941 / 0.3099\n",
      "{'loss_G': 1.8864712745802745, 'loss_D': 1.2506093176773614, 'D_x': 0.6100757721492223, 'D_G_z1': 0.3930542925319501, 'D_G_z2': 0.22861325846718888}\n",
      "[3][49/391]\tLoss_G: 2.0821\tLoss_D: 0.8730\tD(x): 0.6089\tD(G(z)): 0.2200 / 0.1528\n",
      "[3][99/391]\tLoss_G: 3.9312\tLoss_D: 0.4997\tD(x): 0.7290\tD(G(z)): 0.1225 / 0.0298\n",
      "[3][149/391]\tLoss_G: 3.1114\tLoss_D: 1.0654\tD(x): 0.6450\tD(G(z)): 0.3964 / 0.0711\n",
      "[3][199/391]\tLoss_G: 3.2071\tLoss_D: 0.6134\tD(x): 0.7331\tD(G(z)): 0.2072 / 0.0529\n",
      "[3][249/391]\tLoss_G: 1.4804\tLoss_D: 1.1636\tD(x): 0.4231\tD(G(z)): 0.1399 / 0.2819\n",
      "[3][299/391]\tLoss_G: 2.4233\tLoss_D: 0.4911\tD(x): 0.8916\tD(G(z)): 0.2979 / 0.0999\n",
      "[3][349/391]\tLoss_G: 1.7558\tLoss_D: 0.9284\tD(x): 0.5625\tD(G(z)): 0.1894 / 0.2145\n",
      "{'loss_G': 2.2729040352787293, 'loss_D': 1.1771951446362903, 'D_x': 0.6322394712695053, 'D_G_z1': 0.37156526454857414, 'D_G_z2': 0.16908939324186315}\n",
      "[4][49/391]\tLoss_G: 1.8852\tLoss_D: 1.1870\tD(x): 0.4747\tD(G(z)): 0.2842 / 0.1811\n",
      "[4][99/391]\tLoss_G: 2.2782\tLoss_D: 0.7393\tD(x): 0.7964\tD(G(z)): 0.3608 / 0.1288\n",
      "[4][149/391]\tLoss_G: 2.7709\tLoss_D: 0.7897\tD(x): 0.9555\tD(G(z)): 0.5017 / 0.0722\n",
      "[4][199/391]\tLoss_G: 2.9303\tLoss_D: 1.3883\tD(x): 0.7083\tD(G(z)): 0.6037 / 0.0664\n",
      "[4][249/391]\tLoss_G: 3.1700\tLoss_D: 1.0554\tD(x): 0.6220\tD(G(z)): 0.3675 / 0.0714\n",
      "[4][299/391]\tLoss_G: 2.8397\tLoss_D: 1.4183\tD(x): 0.6772\tD(G(z)): 0.5604 / 0.0809\n",
      "[4][349/391]\tLoss_G: 2.1949\tLoss_D: 1.1982\tD(x): 0.3727\tD(G(z)): 0.0921 / 0.1489\n",
      "{'loss_G': 2.244189921958106, 'loss_D': 1.1517058358022145, 'D_x': 0.6359253547447069, 'D_G_z1': 0.36577934586043864, 'D_G_z2': 0.1657978649330991}\n",
      "[5][49/391]\tLoss_G: 2.2828\tLoss_D: 0.6132\tD(x): 0.8201\tD(G(z)): 0.2953 / 0.1207\n",
      "[5][99/391]\tLoss_G: 4.6674\tLoss_D: 0.5517\tD(x): 0.8611\tD(G(z)): 0.3057 / 0.0125\n",
      "[5][149/391]\tLoss_G: 0.8152\tLoss_D: 0.8369\tD(x): 0.5888\tD(G(z)): 0.2027 / 0.4839\n",
      "[5][199/391]\tLoss_G: 2.9462\tLoss_D: 1.0003\tD(x): 0.7508\tD(G(z)): 0.4678 / 0.0625\n",
      "[5][249/391]\tLoss_G: 1.1779\tLoss_D: 1.0535\tD(x): 0.5270\tD(G(z)): 0.2479 / 0.3487\n",
      "[5][299/391]\tLoss_G: 3.9782\tLoss_D: 0.1827\tD(x): 0.9311\tD(G(z)): 0.0953 / 0.0270\n",
      "[5][349/391]\tLoss_G: 1.0946\tLoss_D: 1.2827\tD(x): 0.8092\tD(G(z)): 0.5948 / 0.3695\n",
      "{'loss_G': 2.710243211963347, 'loss_D': 1.0626468975416252, 'D_x': 0.6764584664095725, 'D_G_z1': 0.32626612776358216, 'D_G_z2': 0.132866296280242}\n",
      "[6][49/391]\tLoss_G: 1.8587\tLoss_D: 1.3441\tD(x): 0.4126\tD(G(z)): 0.2208 / 0.2017\n",
      "[6][99/391]\tLoss_G: 3.9150\tLoss_D: 1.9296\tD(x): 0.9808\tD(G(z)): 0.7839 / 0.0313\n",
      "[6][149/391]\tLoss_G: 2.7642\tLoss_D: 0.8912\tD(x): 0.8134\tD(G(z)): 0.4393 / 0.0780\n",
      "[6][199/391]\tLoss_G: 2.2847\tLoss_D: 1.0766\tD(x): 0.4583\tD(G(z)): 0.1303 / 0.1627\n",
      "[6][249/391]\tLoss_G: 1.9815\tLoss_D: 0.8952\tD(x): 0.5333\tD(G(z)): 0.1340 / 0.1848\n",
      "[6][299/391]\tLoss_G: 5.1060\tLoss_D: 1.2024\tD(x): 0.7865\tD(G(z)): 0.5284 / 0.0104\n",
      "[6][349/391]\tLoss_G: 5.5544\tLoss_D: 0.3772\tD(x): 0.9104\tD(G(z)): 0.2255 / 0.0055\n",
      "{'loss_G': 2.897992113402912, 'loss_D': 0.9897746693236488, 'D_x': 0.6965238842687437, 'D_G_z1': 0.3068026288099853, 'D_G_z2': 0.11238419662865842}\n",
      "[7][49/391]\tLoss_G: 2.3895\tLoss_D: 1.0792\tD(x): 0.5270\tD(G(z)): 0.2419 / 0.1075\n",
      "[7][99/391]\tLoss_G: 3.4140\tLoss_D: 0.9194\tD(x): 0.8109\tD(G(z)): 0.4558 / 0.0472\n",
      "[7][149/391]\tLoss_G: 0.4116\tLoss_D: 1.2179\tD(x): 0.4070\tD(G(z)): 0.0554 / 0.7015\n",
      "[7][199/391]\tLoss_G: 1.0485\tLoss_D: 2.1595\tD(x): 0.1680\tD(G(z)): 0.0249 / 0.4066\n",
      "[7][249/391]\tLoss_G: 3.0446\tLoss_D: 0.3368\tD(x): 0.8252\tD(G(z)): 0.1115 / 0.0630\n",
      "[7][299/391]\tLoss_G: 1.7413\tLoss_D: 0.8419\tD(x): 0.6531\tD(G(z)): 0.2919 / 0.2025\n",
      "[7][349/391]\tLoss_G: 4.0110\tLoss_D: 3.2806\tD(x): 0.9842\tD(G(z)): 0.9376 / 0.0273\n",
      "{'loss_G': 2.81592729985714, 'loss_D': 1.0615644812583922, 'D_x': 0.6767696865115845, 'D_G_z1': 0.32543701837637595, 'D_G_z2': 0.12482977893968512}\n",
      "[8][49/391]\tLoss_G: 2.1494\tLoss_D: 0.8928\tD(x): 0.6343\tD(G(z)): 0.2783 / 0.1513\n",
      "[8][99/391]\tLoss_G: 1.5325\tLoss_D: 0.8376\tD(x): 0.7154\tD(G(z)): 0.3643 / 0.2354\n",
      "[8][149/391]\tLoss_G: 2.3883\tLoss_D: 0.8234\tD(x): 0.5497\tD(G(z)): 0.1215 / 0.1086\n",
      "[8][199/391]\tLoss_G: 4.4802\tLoss_D: 0.4624\tD(x): 0.9508\tD(G(z)): 0.3046 / 0.0161\n",
      "[8][249/391]\tLoss_G: 4.7760\tLoss_D: 1.2517\tD(x): 0.7866\tD(G(z)): 0.5639 / 0.0150\n",
      "[8][299/391]\tLoss_G: 1.9308\tLoss_D: 0.7929\tD(x): 0.6881\tD(G(z)): 0.2767 / 0.1760\n",
      "[8][349/391]\tLoss_G: 2.2381\tLoss_D: 0.9614\tD(x): 0.6685\tD(G(z)): 0.3188 / 0.1500\n",
      "{'loss_G': 2.861721502457346, 'loss_D': 1.0464543393680028, 'D_x': 0.6782079727947712, 'D_G_z1': 0.3213964783453516, 'D_G_z2': 0.12802631379404505}\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, epochs + 1):\n",
    "\tgan_hist = fit_dcgan(netG, netD, device, celeba_dl, criterion, e, optimizerG, optimizerD, nz, log_interval=log_interval)\n",
    "\n",
    "\tfor stat in gan_hist:\n",
    "\t\tgan_hist[stat] = np.sum(gan_hist[stat]) / len(gan_hist[stat])\n",
    "\tprint(gan_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(netG, device, 2, save=True, latent_vecs_batch=fixed_noise)\n",
    "# pytorch_dcgan_results_1702854757982334094.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([64, 3, 64, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgs), imgs.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsnew = np.transpose(torchvision.utils.make_grid(imgs, padding=5, normalize=True).cpu(),(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([557, 557, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgsnew), imgsnew.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float32'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgsnew.numpy()), imgs.numpy().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,intermediate_channels,expansion,is_Bottleneck,stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a Bottleneck with conv 1x1->3x3->1x1 layers.\n",
    "        \n",
    "        Note:\n",
    "          1. Addition of feature maps occur at just before the final ReLU with the input feature maps\n",
    "          2. if input size is different from output, select projected mapping or else identity mapping.\n",
    "          3. if is_Bottleneck=False (3x3->3x3) are used else (1x1->3x3->1x1). Bottleneck is required for resnet-50/101/152\n",
    "        Args:\n",
    "            in_channels (int) : input channels to the Bottleneck\n",
    "            intermediate_channels (int) : number of channels to 3x3 conv \n",
    "            expansion (int) : factor by which the input #channels are increased\n",
    "            stride (int) : stride applied in the 3x3 conv. 2 for first Bottleneck of the block and 1 for remaining\n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(Bottleneck,self).__init__()\n",
    "\n",
    "        self.expansion = expansion\n",
    "        self.in_channels = in_channels\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "        self.is_Bottleneck = is_Bottleneck\n",
    "        \n",
    "        # i.e. if dim(x) == dim(F) => Identity function\n",
    "        if self.in_channels==self.intermediate_channels*self.expansion:\n",
    "            self.identity = True\n",
    "        else:\n",
    "            self.identity = False\n",
    "            projection_layer = []\n",
    "            projection_layer.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=stride, padding=0, bias=False ))\n",
    "            projection_layer.append(nn.BatchNorm2d(self.intermediate_channels*self.expansion))\n",
    "            # Only conv->BN and no ReLU\n",
    "            # projection_layer.append(nn.ReLU())\n",
    "            self.projection = nn.Sequential(*projection_layer)\n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # is_Bottleneck = True for all ResNet 50+\n",
    "        if self.is_Bottleneck:\n",
    "            # bottleneck\n",
    "            # 1x1\n",
    "            self.conv1_1x1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 1x1\n",
    "            self.conv3_1x1 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm3 = nn.BatchNorm2d( self.intermediate_channels*self.expansion )\n",
    "        \n",
    "        else:\n",
    "            # basicblock\n",
    "            # 3x3\n",
    "            self.conv1_3x3 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # input stored to be added before the final relu\n",
    "        in_x = x\n",
    "\n",
    "        if self.is_Bottleneck:\n",
    "            # conv1x1->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_1x1(x)))\n",
    "            \n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm2(self.conv2_3x3(x)))\n",
    "            \n",
    "            # conv1x1->BN\n",
    "            x = self.batchnorm3(self.conv3_1x1(x))\n",
    "        \n",
    "        else:\n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_3x3(x)))\n",
    "\n",
    "            # conv3x3->BN\n",
    "            x = self.batchnorm2(self.conv2_3x3(x))\n",
    "\n",
    "\n",
    "        # identity or projected mapping\n",
    "        if self.identity:\n",
    "            x += in_x\n",
    "        else:\n",
    "            x += self.projection(in_x)\n",
    "\n",
    "        # final relu\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Bottleneck(64*4,64,4,stride=1)\n",
    "\n",
    "def test_Bottleneck():\n",
    "    x = torch.randn(1,64,112,112)\n",
    "    model = Bottleneck(64,64,4,True,2)\n",
    "    print(model(x).shape)\n",
    "    del model\n",
    "\n",
    "test_Bottleneck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_variant,in_channels,num_classes):\n",
    "        \"\"\"\n",
    "        Creates the ResNet architecture based on the provided variant. 18/34/50/101 etc.\n",
    "        Based on the input parameters, define the channels list, repeatition list along with expansion factor(4) and stride(3/1)\n",
    "        using _make_blocks method, create a sequence of multiple Bottlenecks\n",
    "        Average Pool at the end before the FC layer \n",
    "\n",
    "        Args:\n",
    "            resnet_variant (list) : eg. [[64,128,256,512],[3,4,6,3],4,True]\n",
    "            in_channels (int) : image channels (3)\n",
    "            num_classes (int) : output #classes \n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "        super(ResNet,self).__init__()\n",
    "        self.channels_list = resnet_variant[0]\n",
    "        self.repeatition_list = resnet_variant[1]\n",
    "        self.expansion = resnet_variant[2]\n",
    "        self.is_Bottleneck = resnet_variant[3]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        # self.block1 = self._make_blocks( 64 , self.channels_list[0], self.repeatition_list[0], self.expansion, self.is_Bottleneck, stride=1 )\n",
    "        self.block1 = nn.Sequential(*[\n",
    "            Bottleneck(64,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block2 = self._make_blocks( self.channels_list[0]*self.expansion , self.channels_list[1], self.repeatition_list[1], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block2 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block3 = self._make_blocks( self.channels_list[1]*self.expansion , self.channels_list[2], self.repeatition_list[2], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block3 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block4 = self._make_blocks( self.channels_list[2]*self.expansion , self.channels_list[3], self.repeatition_list[3], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block4 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[3] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[3] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear( self.channels_list[3]*self.expansion , num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        x = self.average_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _make_blocks(self,in_channels,intermediate_channels,num_repeat, expansion, is_Bottleneck, stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels : #channels of the Bottleneck input\n",
    "            intermediate_channels : #channels of the 3x3 in the Bottleneck\n",
    "            num_repeat : #Bottlenecks in the block\n",
    "            expansion : factor by which intermediate_channels are multiplied to create the output channels\n",
    "            is_Bottleneck : status if Bottleneck in required\n",
    "            stride : stride to be used in the first Bottleneck conv 3x3\n",
    "\n",
    "        Attributes:\n",
    "            Sequence of Bottleneck layers\n",
    "\n",
    "        \"\"\"\n",
    "        layers = [] \n",
    "\n",
    "        layers.append(Bottleneck(in_channels,intermediate_channels,expansion,is_Bottleneck,stride=stride))\n",
    "        for num in range(1,num_repeat):\n",
    "            layers.append(Bottleneck(intermediate_channels*expansion,intermediate_channels,expansion,is_Bottleneck,stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test_ResNet(params):\n",
    "    model = ResNet( params , in_channels=3, num_classes=1000)\n",
    "    x = torch.randn(1,3,224,224)\n",
    "    output = model(x)\n",
    "    print(output.shape)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_parameters={}\n",
    "model_parameters['resnet18'] = ([64,128,256,512],[2,2,2,2],1,False)\n",
    "model_parameters['resnet34'] = ([64,128,256,512],[3,4,6,3],1,False)\n",
    "model_parameters['resnet50'] = ([64,128,256,512],[3,4,6,3],4,True)\n",
    "model_parameters['resnet101'] = ([64,128,256,512],[3,4,23,3],4,True)\n",
    "model_parameters['resnet152'] = ([64,128,256,512],[3,8,36,3],4,True)\n",
    "\n",
    "architecture = 'resnet50'\n",
    "model = test_ResNet(model_parameters[architecture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 800]         628,000\n",
      "            Linear-2                   [-1, 10]           8,010\n",
      "================================================================\n",
      "Total params: 636,010\n",
      "Trainable params: 636,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 2.43\n",
      "Estimated Total Size (MB): 2.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class FCNet(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, layers=[784, 800, 10]):\n",
    "\t\tsuper(FCNet, self).__init__()\n",
    "\t\tself.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(layers[:-1], layers[1:])])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor layer in self.layers[:-1]:\n",
    "\t\t\tx = F.relu(layer(x))\n",
    "\t\tx = self.layers[-1](x)\n",
    "\t\treturn F.log_softmax(x, dim=1)\n",
    "\n",
    "train_dl, _, _ = get_mnist_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "fcnet = FCNet()\n",
    "fcnet = fcnet.to(device)\n",
    "summary(fcnet, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                  [-1, 500]         784,500\n",
      "            Linear-8                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 802,758\n",
      "Trainable params: 802,758\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.33\n",
      "Params size (MB): 3.06\n",
      "Estimated Total Size (MB): 3.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SCVNet(nn.Module):\n",
    "\n",
    "\tdef __init__(self, num_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(1, 16, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(),                                       \n",
    "\t\t\tnn.MaxPool2d(2)\n",
    "\t\t)\n",
    "\t\tself.conv2 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(16, 32, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),  \n",
    "\t\t)\n",
    "\t\tself.dense = nn.Linear(32 * 7 * 7, 500)\n",
    "\t\tself.classifier = nn.Linear(500, num_classes)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.dense(x))\n",
    "\t\treturn F.log_softmax(self.classifier(x), dim=1)\n",
    "\t\n",
    "train_dl, _, _ = get_mnist_loaders(128, flatten=False)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "print(input_size)\n",
    "\n",
    "scvnet = SCVNet()\n",
    "scvnet = scvnet.to(device)\n",
    "summary(scvnet, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 512, 4, 4]         819,200\n",
      "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
      "              ReLU-3            [-1, 512, 4, 4]               0\n",
      "   ConvTranspose2d-4            [-1, 256, 8, 8]       2,097,152\n",
      "       BatchNorm2d-5            [-1, 256, 8, 8]             512\n",
      "              ReLU-6            [-1, 256, 8, 8]               0\n",
      "   ConvTranspose2d-7          [-1, 128, 16, 16]         524,288\n",
      "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-10           [-1, 64, 32, 32]         131,072\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-13            [-1, 3, 64, 64]           3,072\n",
      "             Tanh-14            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 3,576,704\n",
      "Trainable params: 3,576,704\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 13.64\n",
      "Estimated Total Size (MB): 16.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(3, 100, 64)\n",
    "gen = gen.to(device)\n",
    "summary(gen, (100, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           3,072\n",
      "         LeakyReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]         131,072\n",
      "       BatchNorm2d-4          [-1, 128, 16, 16]             256\n",
      "         LeakyReLU-5          [-1, 128, 16, 16]               0\n",
      "            Conv2d-6            [-1, 256, 8, 8]         524,288\n",
      "       BatchNorm2d-7            [-1, 256, 8, 8]             512\n",
      "         LeakyReLU-8            [-1, 256, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 4, 4]       2,097,152\n",
      "      BatchNorm2d-10            [-1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-11            [-1, 512, 4, 4]               0\n",
      "           Conv2d-12              [-1, 1, 1, 1]           8,192\n",
      "          Sigmoid-13              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,765,568\n",
      "Trainable params: 2,765,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.31\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 12.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator(3, 64)\n",
    "disc = disc.to(device)\n",
    "summary(disc, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_noise = torch.randn(32, 100, 1, 1, device=device)\n",
    "print(fixed_noise.shape)\n",
    "gen(fixed_noise).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [-1, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
      "             ReLU-93            [-1, 256, 2, 2]               0\n",
      "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
      "             ReLU-96            [-1, 256, 2, 2]               0\n",
      "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
      "            ReLU-103            [-1, 256, 2, 2]               0\n",
      "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
      "            ReLU-106            [-1, 256, 2, 2]               0\n",
      "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
      "            ReLU-113            [-1, 256, 2, 2]               0\n",
      "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
      "            ReLU-116            [-1, 256, 2, 2]               0\n",
      "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
      "            ReLU-123            [-1, 256, 2, 2]               0\n",
      "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
      "            ReLU-126            [-1, 256, 2, 2]               0\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-143            [-1, 512, 2, 2]               0\n",
      "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-146            [-1, 512, 1, 1]               0\n",
      "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-158            [-1, 512, 1, 1]               0\n",
      "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-165            [-1, 512, 1, 1]               0\n",
      "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-168            [-1, 512, 1, 1]               0\n",
      "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.86\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 95.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rn = resnet50()\n",
    "rn.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "rn = rn.to(device)\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(rn, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [-1, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
      "             ReLU-93            [-1, 256, 2, 2]               0\n",
      "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
      "             ReLU-96            [-1, 256, 2, 2]               0\n",
      "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
      "            ReLU-103            [-1, 256, 2, 2]               0\n",
      "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
      "            ReLU-106            [-1, 256, 2, 2]               0\n",
      "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
      "            ReLU-113            [-1, 256, 2, 2]               0\n",
      "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
      "            ReLU-116            [-1, 256, 2, 2]               0\n",
      "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
      "            ReLU-123            [-1, 256, 2, 2]               0\n",
      "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
      "            ReLU-126            [-1, 256, 2, 2]               0\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-143            [-1, 512, 2, 2]               0\n",
      "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-146            [-1, 512, 1, 1]               0\n",
      "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-158            [-1, 512, 1, 1]               0\n",
      "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-165            [-1, 512, 1, 1]               0\n",
      "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-168            [-1, 512, 1, 1]               0\n",
      "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.86\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 95.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rn_native = ResNet(model_parameters['resnet50'], in_channels=3, num_classes=10)\n",
    "rn_native = rn_native.to(device)\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(rn_native, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-2           [-1, 32, 16, 16]              64\n",
      "             ReLU6-3           [-1, 32, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-5           [-1, 32, 16, 16]              64\n",
      "             ReLU6-6           [-1, 32, 16, 16]               0\n",
      "            Conv2d-7           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-8           [-1, 16, 16, 16]              32\n",
      "  InvertedResidual-9           [-1, 16, 16, 16]               0\n",
      "           Conv2d-10           [-1, 96, 16, 16]           1,536\n",
      "      BatchNorm2d-11           [-1, 96, 16, 16]             192\n",
      "            ReLU6-12           [-1, 96, 16, 16]               0\n",
      "           Conv2d-13             [-1, 96, 8, 8]             864\n",
      "      BatchNorm2d-14             [-1, 96, 8, 8]             192\n",
      "            ReLU6-15             [-1, 96, 8, 8]               0\n",
      "           Conv2d-16             [-1, 24, 8, 8]           2,304\n",
      "      BatchNorm2d-17             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-18             [-1, 24, 8, 8]               0\n",
      "           Conv2d-19            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-20            [-1, 144, 8, 8]             288\n",
      "            ReLU6-21            [-1, 144, 8, 8]               0\n",
      "           Conv2d-22            [-1, 144, 8, 8]           1,296\n",
      "      BatchNorm2d-23            [-1, 144, 8, 8]             288\n",
      "            ReLU6-24            [-1, 144, 8, 8]               0\n",
      "           Conv2d-25             [-1, 24, 8, 8]           3,456\n",
      "      BatchNorm2d-26             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-27             [-1, 24, 8, 8]               0\n",
      "           Conv2d-28            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-29            [-1, 144, 8, 8]             288\n",
      "            ReLU6-30            [-1, 144, 8, 8]               0\n",
      "           Conv2d-31            [-1, 144, 4, 4]           1,296\n",
      "      BatchNorm2d-32            [-1, 144, 4, 4]             288\n",
      "            ReLU6-33            [-1, 144, 4, 4]               0\n",
      "           Conv2d-34             [-1, 32, 4, 4]           4,608\n",
      "      BatchNorm2d-35             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-36             [-1, 32, 4, 4]               0\n",
      "           Conv2d-37            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-38            [-1, 192, 4, 4]             384\n",
      "            ReLU6-39            [-1, 192, 4, 4]               0\n",
      "           Conv2d-40            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-41            [-1, 192, 4, 4]             384\n",
      "            ReLU6-42            [-1, 192, 4, 4]               0\n",
      "           Conv2d-43             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-44             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-45             [-1, 32, 4, 4]               0\n",
      "           Conv2d-46            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-47            [-1, 192, 4, 4]             384\n",
      "            ReLU6-48            [-1, 192, 4, 4]               0\n",
      "           Conv2d-49            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-50            [-1, 192, 4, 4]             384\n",
      "            ReLU6-51            [-1, 192, 4, 4]               0\n",
      "           Conv2d-52             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-53             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-54             [-1, 32, 4, 4]               0\n",
      "           Conv2d-55            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-56            [-1, 192, 4, 4]             384\n",
      "            ReLU6-57            [-1, 192, 4, 4]               0\n",
      "           Conv2d-58            [-1, 192, 2, 2]           1,728\n",
      "      BatchNorm2d-59            [-1, 192, 2, 2]             384\n",
      "            ReLU6-60            [-1, 192, 2, 2]               0\n",
      "           Conv2d-61             [-1, 64, 2, 2]          12,288\n",
      "      BatchNorm2d-62             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-63             [-1, 64, 2, 2]               0\n",
      "           Conv2d-64            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-65            [-1, 384, 2, 2]             768\n",
      "            ReLU6-66            [-1, 384, 2, 2]               0\n",
      "           Conv2d-67            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-68            [-1, 384, 2, 2]             768\n",
      "            ReLU6-69            [-1, 384, 2, 2]               0\n",
      "           Conv2d-70             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-71             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-72             [-1, 64, 2, 2]               0\n",
      "           Conv2d-73            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-74            [-1, 384, 2, 2]             768\n",
      "            ReLU6-75            [-1, 384, 2, 2]               0\n",
      "           Conv2d-76            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-77            [-1, 384, 2, 2]             768\n",
      "            ReLU6-78            [-1, 384, 2, 2]               0\n",
      "           Conv2d-79             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-81             [-1, 64, 2, 2]               0\n",
      "           Conv2d-82            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 2, 2]             768\n",
      "            ReLU6-84            [-1, 384, 2, 2]               0\n",
      "           Conv2d-85            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-86            [-1, 384, 2, 2]             768\n",
      "            ReLU6-87            [-1, 384, 2, 2]               0\n",
      "           Conv2d-88             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-89             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-90             [-1, 64, 2, 2]               0\n",
      "           Conv2d-91            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-92            [-1, 384, 2, 2]             768\n",
      "            ReLU6-93            [-1, 384, 2, 2]               0\n",
      "           Conv2d-94            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-95            [-1, 384, 2, 2]             768\n",
      "            ReLU6-96            [-1, 384, 2, 2]               0\n",
      "           Conv2d-97             [-1, 96, 2, 2]          36,864\n",
      "      BatchNorm2d-98             [-1, 96, 2, 2]             192\n",
      " InvertedResidual-99             [-1, 96, 2, 2]               0\n",
      "          Conv2d-100            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-101            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-102            [-1, 576, 2, 2]               0\n",
      "          Conv2d-103            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-104            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-105            [-1, 576, 2, 2]               0\n",
      "          Conv2d-106             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-107             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-108             [-1, 96, 2, 2]               0\n",
      "          Conv2d-109            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-110            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-111            [-1, 576, 2, 2]               0\n",
      "          Conv2d-112            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-113            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-114            [-1, 576, 2, 2]               0\n",
      "          Conv2d-115             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-116             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-117             [-1, 96, 2, 2]               0\n",
      "          Conv2d-118            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-119            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-120            [-1, 576, 2, 2]               0\n",
      "          Conv2d-121            [-1, 576, 1, 1]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 1, 1]           1,152\n",
      "           ReLU6-123            [-1, 576, 1, 1]               0\n",
      "          Conv2d-124            [-1, 160, 1, 1]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-126            [-1, 160, 1, 1]               0\n",
      "          Conv2d-127            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-129            [-1, 960, 1, 1]               0\n",
      "          Conv2d-130            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-132            [-1, 960, 1, 1]               0\n",
      "          Conv2d-133            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-135            [-1, 160, 1, 1]               0\n",
      "          Conv2d-136            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-138            [-1, 960, 1, 1]               0\n",
      "          Conv2d-139            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-141            [-1, 960, 1, 1]               0\n",
      "          Conv2d-142            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-144            [-1, 160, 1, 1]               0\n",
      "          Conv2d-145            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-147            [-1, 960, 1, 1]               0\n",
      "          Conv2d-148            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-150            [-1, 960, 1, 1]               0\n",
      "          Conv2d-151            [-1, 320, 1, 1]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 1, 1]             640\n",
      "InvertedResidual-153            [-1, 320, 1, 1]               0\n",
      "          Conv2d-154           [-1, 1280, 1, 1]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 1, 1]           2,560\n",
      "           ReLU6-156           [-1, 1280, 1, 1]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                   [-1, 10]          12,810\n",
      "================================================================\n",
      "Total params: 2,236,682\n",
      "Trainable params: 2,236,682\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.13\n",
      "Params size (MB): 8.53\n",
      "Estimated Total Size (MB): 11.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = mobilenet_v2()\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 3, 64, 64]), 2111)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba = get_celeba_loader(batch_size=96)\n",
    "next(iter(celeba))[0].shape, len(celeba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(3, 100, 64)\n",
    "gen = gen.to(device)\n",
    "disc = Discriminator(3, 64)\n",
    "disc = disc.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
