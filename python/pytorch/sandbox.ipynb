{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "from clf_funcs import fit, test, get_cifar10_loaders, get_mnist_loaders, FullyConnectedNet, SimpleConvNet\n",
    "from dcgan_funcs import fit_dcgan, get_celeba_loader, get_celeba_loader_from_memory, Discriminator, Generator, dcgan_weights_init, generate\n",
    "from sodnet_funcs import SODNet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce\n",
    "flat_map = lambda f, xs: reduce(lambda a, b: a + b, map(f, xs), [])\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import mobilenet_v2, resnet50, densenet121, convnext_tiny\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch no. 0\n",
      "processing batch no. 50\n",
      "processing batch no. 100\n",
      "processing batch no. 150\n",
      "processing batch no. 200\n",
      "processing batch no. 250\n",
      "processing batch no. 300\n",
      "processing batch no. 350\n",
      "processing batch no. 400\n",
      "processing batch no. 450\n",
      "processing batch no. 500\n"
     ]
    }
   ],
   "source": [
    "dl = get_celeba_loader(96, root='../../datasets/celeba_trunc/')\n",
    "all_batches = [batch[0] for batch in dl]\n",
    "all_batches = []\n",
    "for i, batch in enumerate(dl):\n",
    "\tif i % 50 == 0: print(f'processing batch no. {i}')\n",
    "\tall_batches.append(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected = torch.concat(all_batches, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celeba_loader_from_memory(batch_size, image_size=64, root='../../datasets/celeba'):\n",
    "\tdl = get_celeba_loader(batch_size, image_size=image_size, root=root)\n",
    "\n",
    "\t# all_batches = [batch[0] for batch in dl]\n",
    "\tcollected_batches = []\n",
    "\tfor i, batch in enumerate(dl):\n",
    "\t\tif i % 50 == 0: print(f'processing batch no. {i}')\n",
    "\t\tcollected_batches.append(batch)\n",
    "\n",
    "\treturn collected_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dl = get_celeba_loader_from_memory(96, root='../../datasets/celeba_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 3, 64, 64]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_dl[0][0].shape, celeba_dl[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: False\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "epochs = 2\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "log_interval = 300\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')\n",
    "\n",
    "# start = torch.cuda.Event(enable_timing=True)\n",
    "# end = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenet_v2()\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mobilenet_v2()\n",
    "# model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "train_dl, test_dl = get_cifar10_loaders(batch_size, test_batch_size)\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "\ttrain_history = fit(model, device, train_dl, loss_func, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\t_, accuracy = test(model, device, test_dl, loss_func, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 3\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "lr = 1e-2\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(nc, nz, ngf).to(device)\n",
    "netD = Discriminator(nc, ndf).to(device)\n",
    "netG.apply(dcgan_weights_init)\n",
    "netD.apply(dcgan_weights_init)\n",
    "\n",
    "# celeba_dl = get_celeba_loader(batch_size=batch_size, root='../../datasets/celeba_trunc/')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1.\n",
    "fake_label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "96\n",
      "torch.Size([3, 64, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \tgan_hist \u001b[39m=\u001b[39m fit_dcgan(netG, netD, device, celeba_dl, criterion, e, optimizerG, optimizerD, nz, log_interval\u001b[39m=\u001b[39;49mlog_interval)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m stat \u001b[39min\u001b[39;00m gan_hist:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \t\tgan_hist[stat] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(gan_hist[stat]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(gan_hist[stat])\n",
      "\u001b[1;32m/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m b_size \u001b[39m=\u001b[39m real_cpu\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b_size,), real_label, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m output \u001b[39m=\u001b[39m discriminator(real_cpu)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m errD_real \u001b[39m=\u001b[39m loss_func(output, label)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m errD_real\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dnn-lang-comparison/python/pytorch/dcgan_funcs.py:179\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:416\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "for e in range(1, epochs + 1):\n",
    "\tgan_hist = fit_dcgan(netG, netD, device, celeba_dl, criterion, e, optimizerG, optimizerD, nz, log_interval=log_interval)\n",
    "\n",
    "\tfor stat in gan_hist:\n",
    "\t\tgan_hist[stat] = np.sum(gan_hist[stat]) / len(gan_hist[stat])\n",
    "\tprint(gan_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(netG, device, 2, save=True, latent_vecs_batch=fixed_noise)\n",
    "# pytorch_dcgan_results_1702854757982334094.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([64, 3, 64, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgs), imgs.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsnew = np.transpose(torchvision.utils.make_grid(imgs, padding=5, normalize=True).cpu(),(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([557, 557, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgsnew), imgsnew.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float32'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgsnew.numpy()), imgs.numpy().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,intermediate_channels,expansion,is_Bottleneck,stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a Bottleneck with conv 1x1->3x3->1x1 layers.\n",
    "        \n",
    "        Note:\n",
    "          1. Addition of feature maps occur at just before the final ReLU with the input feature maps\n",
    "          2. if input size is different from output, select projected mapping or else identity mapping.\n",
    "          3. if is_Bottleneck=False (3x3->3x3) are used else (1x1->3x3->1x1). Bottleneck is required for resnet-50/101/152\n",
    "        Args:\n",
    "            in_channels (int) : input channels to the Bottleneck\n",
    "            intermediate_channels (int) : number of channels to 3x3 conv \n",
    "            expansion (int) : factor by which the input #channels are increased\n",
    "            stride (int) : stride applied in the 3x3 conv. 2 for first Bottleneck of the block and 1 for remaining\n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(Bottleneck,self).__init__()\n",
    "\n",
    "        self.expansion = expansion\n",
    "        self.in_channels = in_channels\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "        self.is_Bottleneck = is_Bottleneck\n",
    "        \n",
    "        # i.e. if dim(x) == dim(F) => Identity function\n",
    "        if self.in_channels==self.intermediate_channels*self.expansion:\n",
    "            self.identity = True\n",
    "        else:\n",
    "            self.identity = False\n",
    "            projection_layer = []\n",
    "            projection_layer.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=stride, padding=0, bias=False ))\n",
    "            projection_layer.append(nn.BatchNorm2d(self.intermediate_channels*self.expansion))\n",
    "            # Only conv->BN and no ReLU\n",
    "            # projection_layer.append(nn.ReLU())\n",
    "            self.projection = nn.Sequential(*projection_layer)\n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # is_Bottleneck = True for all ResNet 50+\n",
    "        if self.is_Bottleneck:\n",
    "            # bottleneck\n",
    "            # 1x1\n",
    "            self.conv1_1x1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 1x1\n",
    "            self.conv3_1x1 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm3 = nn.BatchNorm2d( self.intermediate_channels*self.expansion )\n",
    "        \n",
    "        else:\n",
    "            # basicblock\n",
    "            # 3x3\n",
    "            self.conv1_3x3 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # input stored to be added before the final relu\n",
    "        in_x = x\n",
    "\n",
    "        if self.is_Bottleneck:\n",
    "            # conv1x1->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_1x1(x)))\n",
    "            \n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm2(self.conv2_3x3(x)))\n",
    "            \n",
    "            # conv1x1->BN\n",
    "            x = self.batchnorm3(self.conv3_1x1(x))\n",
    "        \n",
    "        else:\n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_3x3(x)))\n",
    "\n",
    "            # conv3x3->BN\n",
    "            x = self.batchnorm2(self.conv2_3x3(x))\n",
    "\n",
    "\n",
    "        # identity or projected mapping\n",
    "        if self.identity:\n",
    "            x += in_x\n",
    "        else:\n",
    "            x += self.projection(in_x)\n",
    "\n",
    "        # final relu\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Bottleneck(64*4,64,4,stride=1)\n",
    "\n",
    "def test_Bottleneck():\n",
    "    x = torch.randn(1,64,112,112)\n",
    "    model = Bottleneck(64,64,4,True,2)\n",
    "    print(model(x).shape)\n",
    "    del model\n",
    "\n",
    "test_Bottleneck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           4,096\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "            Conv2d-7          [-1, 256, 56, 56]          16,384\n",
      "       BatchNorm2d-8          [-1, 256, 56, 56]             512\n",
      "            Conv2d-9          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 56, 56]             512\n",
      "             ReLU-11          [-1, 256, 56, 56]               0\n",
      "================================================================\n",
      "Total params: 75,008\n",
      "Trainable params: 75,008\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.06\n",
      "Forward/backward pass size (MB): 53.59\n",
      "Params size (MB): 0.29\n",
      "Estimated Total Size (MB): 56.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Bottleneck(64,64,4,True,2)\n",
    "model.to(device)\n",
    "\n",
    "summary(model, (64, 112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_variant,in_channels,num_classes):\n",
    "        \"\"\"\n",
    "        Creates the ResNet architecture based on the provided variant. 18/34/50/101 etc.\n",
    "        Based on the input parameters, define the channels list, repeatition list along with expansion factor(4) and stride(3/1)\n",
    "        using _make_blocks method, create a sequence of multiple Bottlenecks\n",
    "        Average Pool at the end before the FC layer \n",
    "\n",
    "        Args:\n",
    "            resnet_variant (list) : eg. [[64,128,256,512],[3,4,6,3],4,True]\n",
    "            in_channels (int) : image channels (3)\n",
    "            num_classes (int) : output #classes \n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "        super(ResNet,self).__init__()\n",
    "        self.channels_list = resnet_variant[0]\n",
    "        self.repeatition_list = resnet_variant[1]\n",
    "        self.expansion = resnet_variant[2]\n",
    "        self.is_Bottleneck = resnet_variant[3]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        # self.block1 = self._make_blocks( 64 , self.channels_list[0], self.repeatition_list[0], self.expansion, self.is_Bottleneck, stride=1 )\n",
    "        self.block1 = nn.Sequential(*[\n",
    "            Bottleneck(64,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block2 = self._make_blocks( self.channels_list[0]*self.expansion , self.channels_list[1], self.repeatition_list[1], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block2 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block3 = self._make_blocks( self.channels_list[1]*self.expansion , self.channels_list[2], self.repeatition_list[2], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block3 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block4 = self._make_blocks( self.channels_list[2]*self.expansion , self.channels_list[3], self.repeatition_list[3], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block4 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[3] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[3] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear( self.channels_list[3]*self.expansion , num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        x = self.average_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _make_blocks(self,in_channels,intermediate_channels,num_repeat, expansion, is_Bottleneck, stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels : #channels of the Bottleneck input\n",
    "            intermediate_channels : #channels of the 3x3 in the Bottleneck\n",
    "            num_repeat : #Bottlenecks in the block\n",
    "            expansion : factor by which intermediate_channels are multiplied to create the output channels\n",
    "            is_Bottleneck : status if Bottleneck in required\n",
    "            stride : stride to be used in the first Bottleneck conv 3x3\n",
    "\n",
    "        Attributes:\n",
    "            Sequence of Bottleneck layers\n",
    "\n",
    "        \"\"\"\n",
    "        layers = [] \n",
    "\n",
    "        layers.append(Bottleneck(in_channels,intermediate_channels,expansion,is_Bottleneck,stride=stride))\n",
    "        for num in range(1,num_repeat):\n",
    "            layers.append(Bottleneck(intermediate_channels*expansion,intermediate_channels,expansion,is_Bottleneck,stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test_ResNet(params):\n",
    "    model = ResNet( params , in_channels=3, num_classes=1000)\n",
    "    x = torch.randn(1,3,224,224)\n",
    "    output = model(x)\n",
    "    print(output.shape)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_parameters={}\n",
    "model_parameters['resnet18'] = ([64,128,256,512],[2,2,2,2],1,False)\n",
    "model_parameters['resnet34'] = ([64,128,256,512],[3,4,6,3],1,False)\n",
    "model_parameters['resnet50'] = ([64,128,256,512],[3,4,6,3],4,True)\n",
    "model_parameters['resnet101'] = ([64,128,256,512],[3,4,23,3],4,True)\n",
    "model_parameters['resnet152'] = ([64,128,256,512],[3,8,36,3],4,True)\n",
    "\n",
    "architecture = 'resnet50'\n",
    "model = test_ResNet(model_parameters[architecture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 800]         628,000\n",
      "            Linear-2                   [-1, 10]           8,010\n",
      "================================================================\n",
      "Total params: 636,010\n",
      "Trainable params: 636,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 2.43\n",
      "Estimated Total Size (MB): 2.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class FCNet(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, layers=[784, 800, 10]):\n",
    "\t\tsuper(FCNet, self).__init__()\n",
    "\t\tself.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(layers[:-1], layers[1:])])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor layer in self.layers[:-1]:\n",
    "\t\t\tx = F.relu(layer(x))\n",
    "\t\tx = self.layers[-1](x)\n",
    "\t\treturn F.log_softmax(x, dim=1)\n",
    "\n",
    "train_dl, _, _ = get_mnist_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "fcnet = FCNet()\n",
    "fcnet = fcnet.to(device)\n",
    "summary(fcnet, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                  [-1, 500]         784,500\n",
      "            Linear-8                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 802,758\n",
      "Trainable params: 802,758\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.33\n",
      "Params size (MB): 3.06\n",
      "Estimated Total Size (MB): 3.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SCVNet(nn.Module):\n",
    "\n",
    "\tdef __init__(self, num_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(1, 16, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(),                                       \n",
    "\t\t\tnn.MaxPool2d(2)\n",
    "\t\t)\n",
    "\t\tself.conv2 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(16, 32, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),  \n",
    "\t\t)\n",
    "\t\tself.dense = nn.Linear(32 * 7 * 7, 500)\n",
    "\t\tself.classifier = nn.Linear(500, num_classes)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.dense(x))\n",
    "\t\treturn F.log_softmax(self.classifier(x), dim=1)\n",
    "\t\n",
    "train_dl, _, _ = get_mnist_loaders(128, flatten=False)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "print(input_size)\n",
    "\n",
    "scvnet = SCVNet()\n",
    "scvnet = scvnet.to(device)\n",
    "summary(scvnet, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 512, 4, 4]         819,200\n",
      "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
      "              ReLU-3            [-1, 512, 4, 4]               0\n",
      "   ConvTranspose2d-4            [-1, 256, 8, 8]       2,097,152\n",
      "       BatchNorm2d-5            [-1, 256, 8, 8]             512\n",
      "              ReLU-6            [-1, 256, 8, 8]               0\n",
      "   ConvTranspose2d-7          [-1, 128, 16, 16]         524,288\n",
      "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-10           [-1, 64, 32, 32]         131,072\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-13            [-1, 3, 64, 64]           3,072\n",
      "             Tanh-14            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 3,576,704\n",
      "Trainable params: 3,576,704\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 13.64\n",
      "Estimated Total Size (MB): 16.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(3, 100, 64)\n",
    "gen = gen.to(device)\n",
    "summary(gen, (100, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           3,072\n",
      "         LeakyReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]         131,072\n",
      "       BatchNorm2d-4          [-1, 128, 16, 16]             256\n",
      "         LeakyReLU-5          [-1, 128, 16, 16]               0\n",
      "            Conv2d-6            [-1, 256, 8, 8]         524,288\n",
      "       BatchNorm2d-7            [-1, 256, 8, 8]             512\n",
      "         LeakyReLU-8            [-1, 256, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 4, 4]       2,097,152\n",
      "      BatchNorm2d-10            [-1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-11            [-1, 512, 4, 4]               0\n",
      "           Conv2d-12              [-1, 1, 1, 1]           8,192\n",
      "          Sigmoid-13              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,765,568\n",
      "Trainable params: 2,765,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.31\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 12.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator(3, 64)\n",
    "disc = disc.to(device)\n",
    "summary(disc, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_noise = torch.randn(32, 100, 1, 1, device=device)\n",
    "print(fixed_noise.shape)\n",
    "gen(fixed_noise).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [-1, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
      "             ReLU-93            [-1, 256, 2, 2]               0\n",
      "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
      "             ReLU-96            [-1, 256, 2, 2]               0\n",
      "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
      "            ReLU-103            [-1, 256, 2, 2]               0\n",
      "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
      "            ReLU-106            [-1, 256, 2, 2]               0\n",
      "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
      "            ReLU-113            [-1, 256, 2, 2]               0\n",
      "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
      "            ReLU-116            [-1, 256, 2, 2]               0\n",
      "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
      "            ReLU-123            [-1, 256, 2, 2]               0\n",
      "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
      "            ReLU-126            [-1, 256, 2, 2]               0\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-143            [-1, 512, 2, 2]               0\n",
      "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-146            [-1, 512, 1, 1]               0\n",
      "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-158            [-1, 512, 1, 1]               0\n",
      "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-165            [-1, 512, 1, 1]               0\n",
      "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-168            [-1, 512, 1, 1]               0\n",
      "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.86\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 95.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rn = resnet50()\n",
    "rn.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "rn = rn.to(device)\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(rn, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [-1, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
      "             ReLU-93            [-1, 256, 2, 2]               0\n",
      "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
      "             ReLU-96            [-1, 256, 2, 2]               0\n",
      "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
      "            ReLU-103            [-1, 256, 2, 2]               0\n",
      "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
      "            ReLU-106            [-1, 256, 2, 2]               0\n",
      "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
      "            ReLU-113            [-1, 256, 2, 2]               0\n",
      "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
      "            ReLU-116            [-1, 256, 2, 2]               0\n",
      "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
      "            ReLU-123            [-1, 256, 2, 2]               0\n",
      "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
      "            ReLU-126            [-1, 256, 2, 2]               0\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-143            [-1, 512, 2, 2]               0\n",
      "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-146            [-1, 512, 1, 1]               0\n",
      "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-158            [-1, 512, 1, 1]               0\n",
      "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-165            [-1, 512, 1, 1]               0\n",
      "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-168            [-1, 512, 1, 1]               0\n",
      "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.86\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 95.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rn_native = ResNet(model_parameters['resnet50'], in_channels=3, num_classes=10)\n",
    "rn_native = rn_native.to(device)\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(rn_native, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-2           [-1, 32, 16, 16]              64\n",
      "             ReLU6-3           [-1, 32, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-5           [-1, 32, 16, 16]              64\n",
      "             ReLU6-6           [-1, 32, 16, 16]               0\n",
      "            Conv2d-7           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-8           [-1, 16, 16, 16]              32\n",
      "  InvertedResidual-9           [-1, 16, 16, 16]               0\n",
      "           Conv2d-10           [-1, 96, 16, 16]           1,536\n",
      "      BatchNorm2d-11           [-1, 96, 16, 16]             192\n",
      "            ReLU6-12           [-1, 96, 16, 16]               0\n",
      "           Conv2d-13             [-1, 96, 8, 8]             864\n",
      "      BatchNorm2d-14             [-1, 96, 8, 8]             192\n",
      "            ReLU6-15             [-1, 96, 8, 8]               0\n",
      "           Conv2d-16             [-1, 24, 8, 8]           2,304\n",
      "      BatchNorm2d-17             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-18             [-1, 24, 8, 8]               0\n",
      "           Conv2d-19            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-20            [-1, 144, 8, 8]             288\n",
      "            ReLU6-21            [-1, 144, 8, 8]               0\n",
      "           Conv2d-22            [-1, 144, 8, 8]           1,296\n",
      "      BatchNorm2d-23            [-1, 144, 8, 8]             288\n",
      "            ReLU6-24            [-1, 144, 8, 8]               0\n",
      "           Conv2d-25             [-1, 24, 8, 8]           3,456\n",
      "      BatchNorm2d-26             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-27             [-1, 24, 8, 8]               0\n",
      "           Conv2d-28            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-29            [-1, 144, 8, 8]             288\n",
      "            ReLU6-30            [-1, 144, 8, 8]               0\n",
      "           Conv2d-31            [-1, 144, 4, 4]           1,296\n",
      "      BatchNorm2d-32            [-1, 144, 4, 4]             288\n",
      "            ReLU6-33            [-1, 144, 4, 4]               0\n",
      "           Conv2d-34             [-1, 32, 4, 4]           4,608\n",
      "      BatchNorm2d-35             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-36             [-1, 32, 4, 4]               0\n",
      "           Conv2d-37            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-38            [-1, 192, 4, 4]             384\n",
      "            ReLU6-39            [-1, 192, 4, 4]               0\n",
      "           Conv2d-40            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-41            [-1, 192, 4, 4]             384\n",
      "            ReLU6-42            [-1, 192, 4, 4]               0\n",
      "           Conv2d-43             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-44             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-45             [-1, 32, 4, 4]               0\n",
      "           Conv2d-46            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-47            [-1, 192, 4, 4]             384\n",
      "            ReLU6-48            [-1, 192, 4, 4]               0\n",
      "           Conv2d-49            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-50            [-1, 192, 4, 4]             384\n",
      "            ReLU6-51            [-1, 192, 4, 4]               0\n",
      "           Conv2d-52             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-53             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-54             [-1, 32, 4, 4]               0\n",
      "           Conv2d-55            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-56            [-1, 192, 4, 4]             384\n",
      "            ReLU6-57            [-1, 192, 4, 4]               0\n",
      "           Conv2d-58            [-1, 192, 2, 2]           1,728\n",
      "      BatchNorm2d-59            [-1, 192, 2, 2]             384\n",
      "            ReLU6-60            [-1, 192, 2, 2]               0\n",
      "           Conv2d-61             [-1, 64, 2, 2]          12,288\n",
      "      BatchNorm2d-62             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-63             [-1, 64, 2, 2]               0\n",
      "           Conv2d-64            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-65            [-1, 384, 2, 2]             768\n",
      "            ReLU6-66            [-1, 384, 2, 2]               0\n",
      "           Conv2d-67            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-68            [-1, 384, 2, 2]             768\n",
      "            ReLU6-69            [-1, 384, 2, 2]               0\n",
      "           Conv2d-70             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-71             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-72             [-1, 64, 2, 2]               0\n",
      "           Conv2d-73            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-74            [-1, 384, 2, 2]             768\n",
      "            ReLU6-75            [-1, 384, 2, 2]               0\n",
      "           Conv2d-76            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-77            [-1, 384, 2, 2]             768\n",
      "            ReLU6-78            [-1, 384, 2, 2]               0\n",
      "           Conv2d-79             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-81             [-1, 64, 2, 2]               0\n",
      "           Conv2d-82            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 2, 2]             768\n",
      "            ReLU6-84            [-1, 384, 2, 2]               0\n",
      "           Conv2d-85            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-86            [-1, 384, 2, 2]             768\n",
      "            ReLU6-87            [-1, 384, 2, 2]               0\n",
      "           Conv2d-88             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-89             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-90             [-1, 64, 2, 2]               0\n",
      "           Conv2d-91            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-92            [-1, 384, 2, 2]             768\n",
      "            ReLU6-93            [-1, 384, 2, 2]               0\n",
      "           Conv2d-94            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-95            [-1, 384, 2, 2]             768\n",
      "            ReLU6-96            [-1, 384, 2, 2]               0\n",
      "           Conv2d-97             [-1, 96, 2, 2]          36,864\n",
      "      BatchNorm2d-98             [-1, 96, 2, 2]             192\n",
      " InvertedResidual-99             [-1, 96, 2, 2]               0\n",
      "          Conv2d-100            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-101            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-102            [-1, 576, 2, 2]               0\n",
      "          Conv2d-103            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-104            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-105            [-1, 576, 2, 2]               0\n",
      "          Conv2d-106             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-107             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-108             [-1, 96, 2, 2]               0\n",
      "          Conv2d-109            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-110            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-111            [-1, 576, 2, 2]               0\n",
      "          Conv2d-112            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-113            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-114            [-1, 576, 2, 2]               0\n",
      "          Conv2d-115             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-116             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-117             [-1, 96, 2, 2]               0\n",
      "          Conv2d-118            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-119            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-120            [-1, 576, 2, 2]               0\n",
      "          Conv2d-121            [-1, 576, 1, 1]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 1, 1]           1,152\n",
      "           ReLU6-123            [-1, 576, 1, 1]               0\n",
      "          Conv2d-124            [-1, 160, 1, 1]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-126            [-1, 160, 1, 1]               0\n",
      "          Conv2d-127            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-129            [-1, 960, 1, 1]               0\n",
      "          Conv2d-130            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-132            [-1, 960, 1, 1]               0\n",
      "          Conv2d-133            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-135            [-1, 160, 1, 1]               0\n",
      "          Conv2d-136            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-138            [-1, 960, 1, 1]               0\n",
      "          Conv2d-139            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-141            [-1, 960, 1, 1]               0\n",
      "          Conv2d-142            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-144            [-1, 160, 1, 1]               0\n",
      "          Conv2d-145            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-147            [-1, 960, 1, 1]               0\n",
      "          Conv2d-148            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-150            [-1, 960, 1, 1]               0\n",
      "          Conv2d-151            [-1, 320, 1, 1]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 1, 1]             640\n",
      "InvertedResidual-153            [-1, 320, 1, 1]               0\n",
      "          Conv2d-154           [-1, 1280, 1, 1]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 1, 1]           2,560\n",
      "           ReLU6-156           [-1, 1280, 1, 1]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                   [-1, 10]          12,810\n",
      "================================================================\n",
      "Total params: 2,236,682\n",
      "Trainable params: 2,236,682\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.13\n",
      "Params size (MB): 8.53\n",
      "Estimated Total Size (MB): 11.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "model = mobilenet_v2()\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,024\n",
      "            Conv2d-2         [-1, 64, 224, 224]             576\n",
      "            Conv2d-3         [-1, 16, 224, 224]           1,024\n",
      "================================================================\n",
      "Total params: 2,624\n",
      "Trainable params: 2,624\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.06\n",
      "Forward/backward pass size (MB): 55.12\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 58.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# mobilenet v2 inverted residual block\n",
    "mobresblock = nn.Sequential(*[\n",
    "\tnn.Conv2d(16, 64, 1, 1, 0, bias=False),\n",
    "\tnn.Conv2d(64, 64, 3, 1, 1, groups=64, bias=False), # depthwise\n",
    "\tnn.Conv2d(64, 16, 1, 1, 0, bias=False),\n",
    "])\n",
    "\n",
    "mobresblock = mobresblock.to(device)\n",
    "summary(mobresblock, (16, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 96, 8, 8]           4,704\n",
      "       LayerNorm2d-2             [-1, 96, 8, 8]             192\n",
      "            Conv2d-3             [-1, 96, 8, 8]           4,800\n",
      "           Permute-4             [-1, 8, 8, 96]               0\n",
      "         LayerNorm-5             [-1, 8, 8, 96]             192\n",
      "            Linear-6            [-1, 8, 8, 384]          37,248\n",
      "              GELU-7            [-1, 8, 8, 384]               0\n",
      "            Linear-8             [-1, 8, 8, 96]          36,960\n",
      "           Permute-9             [-1, 96, 8, 8]               0\n",
      "  StochasticDepth-10             [-1, 96, 8, 8]               0\n",
      "          CNBlock-11             [-1, 96, 8, 8]               0\n",
      "           Conv2d-12             [-1, 96, 8, 8]           4,800\n",
      "          Permute-13             [-1, 8, 8, 96]               0\n",
      "        LayerNorm-14             [-1, 8, 8, 96]             192\n",
      "           Linear-15            [-1, 8, 8, 384]          37,248\n",
      "             GELU-16            [-1, 8, 8, 384]               0\n",
      "           Linear-17             [-1, 8, 8, 96]          36,960\n",
      "          Permute-18             [-1, 96, 8, 8]               0\n",
      "  StochasticDepth-19             [-1, 96, 8, 8]               0\n",
      "          CNBlock-20             [-1, 96, 8, 8]               0\n",
      "           Conv2d-21             [-1, 96, 8, 8]           4,800\n",
      "          Permute-22             [-1, 8, 8, 96]               0\n",
      "        LayerNorm-23             [-1, 8, 8, 96]             192\n",
      "           Linear-24            [-1, 8, 8, 384]          37,248\n",
      "             GELU-25            [-1, 8, 8, 384]               0\n",
      "           Linear-26             [-1, 8, 8, 96]          36,960\n",
      "          Permute-27             [-1, 96, 8, 8]               0\n",
      "  StochasticDepth-28             [-1, 96, 8, 8]               0\n",
      "          CNBlock-29             [-1, 96, 8, 8]               0\n",
      "      LayerNorm2d-30             [-1, 96, 8, 8]             192\n",
      "           Conv2d-31            [-1, 192, 4, 4]          73,920\n",
      "           Conv2d-32            [-1, 192, 4, 4]           9,600\n",
      "          Permute-33            [-1, 4, 4, 192]               0\n",
      "        LayerNorm-34            [-1, 4, 4, 192]             384\n",
      "           Linear-35            [-1, 4, 4, 768]         148,224\n",
      "             GELU-36            [-1, 4, 4, 768]               0\n",
      "           Linear-37            [-1, 4, 4, 192]         147,648\n",
      "          Permute-38            [-1, 192, 4, 4]               0\n",
      "  StochasticDepth-39            [-1, 192, 4, 4]               0\n",
      "          CNBlock-40            [-1, 192, 4, 4]               0\n",
      "           Conv2d-41            [-1, 192, 4, 4]           9,600\n",
      "          Permute-42            [-1, 4, 4, 192]               0\n",
      "        LayerNorm-43            [-1, 4, 4, 192]             384\n",
      "           Linear-44            [-1, 4, 4, 768]         148,224\n",
      "             GELU-45            [-1, 4, 4, 768]               0\n",
      "           Linear-46            [-1, 4, 4, 192]         147,648\n",
      "          Permute-47            [-1, 192, 4, 4]               0\n",
      "  StochasticDepth-48            [-1, 192, 4, 4]               0\n",
      "          CNBlock-49            [-1, 192, 4, 4]               0\n",
      "           Conv2d-50            [-1, 192, 4, 4]           9,600\n",
      "          Permute-51            [-1, 4, 4, 192]               0\n",
      "        LayerNorm-52            [-1, 4, 4, 192]             384\n",
      "           Linear-53            [-1, 4, 4, 768]         148,224\n",
      "             GELU-54            [-1, 4, 4, 768]               0\n",
      "           Linear-55            [-1, 4, 4, 192]         147,648\n",
      "          Permute-56            [-1, 192, 4, 4]               0\n",
      "  StochasticDepth-57            [-1, 192, 4, 4]               0\n",
      "          CNBlock-58            [-1, 192, 4, 4]               0\n",
      "      LayerNorm2d-59            [-1, 192, 4, 4]             384\n",
      "           Conv2d-60            [-1, 384, 2, 2]         295,296\n",
      "           Conv2d-61            [-1, 384, 2, 2]          19,200\n",
      "          Permute-62            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-63            [-1, 2, 2, 384]             768\n",
      "           Linear-64           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-65           [-1, 2, 2, 1536]               0\n",
      "           Linear-66            [-1, 2, 2, 384]         590,208\n",
      "          Permute-67            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-68            [-1, 384, 2, 2]               0\n",
      "          CNBlock-69            [-1, 384, 2, 2]               0\n",
      "           Conv2d-70            [-1, 384, 2, 2]          19,200\n",
      "          Permute-71            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-72            [-1, 2, 2, 384]             768\n",
      "           Linear-73           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-74           [-1, 2, 2, 1536]               0\n",
      "           Linear-75            [-1, 2, 2, 384]         590,208\n",
      "          Permute-76            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-77            [-1, 384, 2, 2]               0\n",
      "          CNBlock-78            [-1, 384, 2, 2]               0\n",
      "           Conv2d-79            [-1, 384, 2, 2]          19,200\n",
      "          Permute-80            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-81            [-1, 2, 2, 384]             768\n",
      "           Linear-82           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-83           [-1, 2, 2, 1536]               0\n",
      "           Linear-84            [-1, 2, 2, 384]         590,208\n",
      "          Permute-85            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-86            [-1, 384, 2, 2]               0\n",
      "          CNBlock-87            [-1, 384, 2, 2]               0\n",
      "           Conv2d-88            [-1, 384, 2, 2]          19,200\n",
      "          Permute-89            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-90            [-1, 2, 2, 384]             768\n",
      "           Linear-91           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-92           [-1, 2, 2, 1536]               0\n",
      "           Linear-93            [-1, 2, 2, 384]         590,208\n",
      "          Permute-94            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-95            [-1, 384, 2, 2]               0\n",
      "          CNBlock-96            [-1, 384, 2, 2]               0\n",
      "           Conv2d-97            [-1, 384, 2, 2]          19,200\n",
      "          Permute-98            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-99            [-1, 2, 2, 384]             768\n",
      "          Linear-100           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-101           [-1, 2, 2, 1536]               0\n",
      "          Linear-102            [-1, 2, 2, 384]         590,208\n",
      "         Permute-103            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-104            [-1, 384, 2, 2]               0\n",
      "         CNBlock-105            [-1, 384, 2, 2]               0\n",
      "          Conv2d-106            [-1, 384, 2, 2]          19,200\n",
      "         Permute-107            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-108            [-1, 2, 2, 384]             768\n",
      "          Linear-109           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-110           [-1, 2, 2, 1536]               0\n",
      "          Linear-111            [-1, 2, 2, 384]         590,208\n",
      "         Permute-112            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-113            [-1, 384, 2, 2]               0\n",
      "         CNBlock-114            [-1, 384, 2, 2]               0\n",
      "          Conv2d-115            [-1, 384, 2, 2]          19,200\n",
      "         Permute-116            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-117            [-1, 2, 2, 384]             768\n",
      "          Linear-118           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-119           [-1, 2, 2, 1536]               0\n",
      "          Linear-120            [-1, 2, 2, 384]         590,208\n",
      "         Permute-121            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-122            [-1, 384, 2, 2]               0\n",
      "         CNBlock-123            [-1, 384, 2, 2]               0\n",
      "          Conv2d-124            [-1, 384, 2, 2]          19,200\n",
      "         Permute-125            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-126            [-1, 2, 2, 384]             768\n",
      "          Linear-127           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-128           [-1, 2, 2, 1536]               0\n",
      "          Linear-129            [-1, 2, 2, 384]         590,208\n",
      "         Permute-130            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-131            [-1, 384, 2, 2]               0\n",
      "         CNBlock-132            [-1, 384, 2, 2]               0\n",
      "          Conv2d-133            [-1, 384, 2, 2]          19,200\n",
      "         Permute-134            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-135            [-1, 2, 2, 384]             768\n",
      "          Linear-136           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-137           [-1, 2, 2, 1536]               0\n",
      "          Linear-138            [-1, 2, 2, 384]         590,208\n",
      "         Permute-139            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-140            [-1, 384, 2, 2]               0\n",
      "         CNBlock-141            [-1, 384, 2, 2]               0\n",
      "     LayerNorm2d-142            [-1, 384, 2, 2]             768\n",
      "          Conv2d-143            [-1, 768, 1, 1]       1,180,416\n",
      "          Conv2d-144            [-1, 768, 1, 1]          38,400\n",
      "         Permute-145            [-1, 1, 1, 768]               0\n",
      "       LayerNorm-146            [-1, 1, 1, 768]           1,536\n",
      "          Linear-147           [-1, 1, 1, 3072]       2,362,368\n",
      "            GELU-148           [-1, 1, 1, 3072]               0\n",
      "          Linear-149            [-1, 1, 1, 768]       2,360,064\n",
      "         Permute-150            [-1, 768, 1, 1]               0\n",
      " StochasticDepth-151            [-1, 768, 1, 1]               0\n",
      "         CNBlock-152            [-1, 768, 1, 1]               0\n",
      "          Conv2d-153            [-1, 768, 1, 1]          38,400\n",
      "         Permute-154            [-1, 1, 1, 768]               0\n",
      "       LayerNorm-155            [-1, 1, 1, 768]           1,536\n",
      "          Linear-156           [-1, 1, 1, 3072]       2,362,368\n",
      "            GELU-157           [-1, 1, 1, 3072]               0\n",
      "          Linear-158            [-1, 1, 1, 768]       2,360,064\n",
      "         Permute-159            [-1, 768, 1, 1]               0\n",
      " StochasticDepth-160            [-1, 768, 1, 1]               0\n",
      "         CNBlock-161            [-1, 768, 1, 1]               0\n",
      "          Conv2d-162            [-1, 768, 1, 1]          38,400\n",
      "         Permute-163            [-1, 1, 1, 768]               0\n",
      "       LayerNorm-164            [-1, 1, 1, 768]           1,536\n",
      "          Linear-165           [-1, 1, 1, 3072]       2,362,368\n",
      "            GELU-166           [-1, 1, 1, 3072]               0\n",
      "          Linear-167            [-1, 1, 1, 768]       2,360,064\n",
      "         Permute-168            [-1, 768, 1, 1]               0\n",
      " StochasticDepth-169            [-1, 768, 1, 1]               0\n",
      "         CNBlock-170            [-1, 768, 1, 1]               0\n",
      "AdaptiveAvgPool2d-171            [-1, 768, 1, 1]               0\n",
      "     LayerNorm2d-172            [-1, 768, 1, 1]           1,536\n",
      "         Flatten-173                  [-1, 768]               0\n",
      "          Linear-174                   [-1, 10]           7,690\n",
      "================================================================\n",
      "Total params: 27,821,194\n",
      "Trainable params: 27,821,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.24\n",
      "Params size (MB): 106.13\n",
      "Estimated Total Size (MB): 111.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = convnext_tiny()\n",
    "model.classifier[2] = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 3, 64, 64]), 2111)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba = get_celeba_loader(batch_size=96)\n",
    "next(iter(celeba))[0].shape, len(celeba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(3, 100, 64)\n",
    "gen = gen.to(device)\n",
    "disc = Discriminator(3, 64)\n",
    "disc = disc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def summary(model, input_size, batch_size=-1, device=\"cuda\"):\n",
    "\n",
    "    def register_hook(module):\n",
    "\n",
    "        def hook(module, input, output):\n",
    "\n",
    "            # DenseNet implementation is cooked and requires special handling\n",
    "            if '_DenseLayer':\n",
    "                input = (input[0][0],)\n",
    "\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "            and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    device = device.lower()\n",
    "    assert device in [\n",
    "        \"cuda\",\n",
    "        \"cpu\",\n",
    "    ], \"Input device is not valid, please specify 'cuda' or 'cpu'\"\n",
    "\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]\n",
    "    # print(type(x[0]))\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    print(line_new)\n",
    "    print(\"================================================================\")\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        print(line_new)\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    print(\"================================================================\")\n",
    "    print(\"Total params: {0:,}\".format(total_params))\n",
    "    print(\"Trainable params: {0:,}\".format(trainable_params))\n",
    "    print(\"Non-trainable params: {0:,}\".format(total_params - trainable_params))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Input size (MB): %0.2f\" % total_input_size)\n",
    "    print(\"Forward/backward pass size (MB): %0.2f\" % total_output_size)\n",
    "    print(\"Params size (MB): %0.2f\" % total_params_size)\n",
    "    print(\"Estimated Total Size (MB): %0.2f\" % total_size)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "       BatchNorm2d-5             [-1, 64, 8, 8]             128\n",
      "              ReLU-6             [-1, 64, 8, 8]               0\n",
      "            Conv2d-7            [-1, 128, 8, 8]           8,192\n",
      "       BatchNorm2d-8            [-1, 128, 8, 8]             256\n",
      "              ReLU-9            [-1, 128, 8, 8]               0\n",
      "           Conv2d-10             [-1, 32, 8, 8]          36,864\n",
      "      _DenseLayer-11             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-12             [-1, 96, 8, 8]             192\n",
      "             ReLU-13             [-1, 96, 8, 8]               0\n",
      "           Conv2d-14            [-1, 128, 8, 8]          12,288\n",
      "      BatchNorm2d-15            [-1, 128, 8, 8]             256\n",
      "             ReLU-16            [-1, 128, 8, 8]               0\n",
      "           Conv2d-17             [-1, 32, 8, 8]          36,864\n",
      "      _DenseLayer-18             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-19            [-1, 128, 8, 8]             256\n",
      "             ReLU-20            [-1, 128, 8, 8]               0\n",
      "           Conv2d-21            [-1, 128, 8, 8]          16,384\n",
      "      BatchNorm2d-22            [-1, 128, 8, 8]             256\n",
      "             ReLU-23            [-1, 128, 8, 8]               0\n",
      "           Conv2d-24             [-1, 32, 8, 8]          36,864\n",
      "      _DenseLayer-25             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-26            [-1, 160, 8, 8]             320\n",
      "             ReLU-27            [-1, 160, 8, 8]               0\n",
      "           Conv2d-28            [-1, 128, 8, 8]          20,480\n",
      "      BatchNorm2d-29            [-1, 128, 8, 8]             256\n",
      "             ReLU-30            [-1, 128, 8, 8]               0\n",
      "           Conv2d-31             [-1, 32, 8, 8]          36,864\n",
      "      _DenseLayer-32             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-33            [-1, 192, 8, 8]             384\n",
      "             ReLU-34            [-1, 192, 8, 8]               0\n",
      "           Conv2d-35            [-1, 128, 8, 8]          24,576\n",
      "      BatchNorm2d-36            [-1, 128, 8, 8]             256\n",
      "             ReLU-37            [-1, 128, 8, 8]               0\n",
      "           Conv2d-38             [-1, 32, 8, 8]          36,864\n",
      "      _DenseLayer-39             [-1, 32, 8, 8]               0\n",
      "      BatchNorm2d-40            [-1, 224, 8, 8]             448\n",
      "             ReLU-41            [-1, 224, 8, 8]               0\n",
      "           Conv2d-42            [-1, 128, 8, 8]          28,672\n",
      "      BatchNorm2d-43            [-1, 128, 8, 8]             256\n",
      "             ReLU-44            [-1, 128, 8, 8]               0\n",
      "           Conv2d-45             [-1, 32, 8, 8]          36,864\n",
      "      _DenseLayer-46             [-1, 32, 8, 8]               0\n",
      "      _DenseBlock-47            [-1, 256, 8, 8]               0\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "             ReLU-49            [-1, 256, 8, 8]               0\n",
      "           Conv2d-50            [-1, 128, 8, 8]          32,768\n",
      "        AvgPool2d-51            [-1, 128, 4, 4]               0\n",
      "      BatchNorm2d-52            [-1, 128, 4, 4]             256\n",
      "             ReLU-53            [-1, 128, 4, 4]               0\n",
      "           Conv2d-54            [-1, 128, 4, 4]          16,384\n",
      "      BatchNorm2d-55            [-1, 128, 4, 4]             256\n",
      "             ReLU-56            [-1, 128, 4, 4]               0\n",
      "           Conv2d-57             [-1, 32, 4, 4]          36,864\n",
      "      _DenseLayer-58             [-1, 32, 4, 4]               0\n",
      "      BatchNorm2d-59            [-1, 160, 4, 4]             320\n",
      "             ReLU-60            [-1, 160, 4, 4]               0\n",
      "           Conv2d-61            [-1, 128, 4, 4]          20,480\n",
      "      BatchNorm2d-62            [-1, 128, 4, 4]             256\n",
      "             ReLU-63            [-1, 128, 4, 4]               0\n",
      "           Conv2d-64             [-1, 32, 4, 4]          36,864\n",
      "      _DenseLayer-65             [-1, 32, 4, 4]               0\n",
      "      BatchNorm2d-66            [-1, 192, 4, 4]             384\n",
      "             ReLU-67            [-1, 192, 4, 4]               0\n",
      "           Conv2d-68            [-1, 128, 4, 4]          24,576\n",
      "      BatchNorm2d-69            [-1, 128, 4, 4]             256\n",
      "             ReLU-70            [-1, 128, 4, 4]               0\n",
      "           Conv2d-71             [-1, 32, 4, 4]          36,864\n",
      "      _DenseLayer-72             [-1, 32, 4, 4]               0\n",
      "      BatchNorm2d-73            [-1, 224, 4, 4]             448\n",
      "             ReLU-74            [-1, 224, 4, 4]               0\n",
      "           Conv2d-75            [-1, 128, 4, 4]          28,672\n",
      "      BatchNorm2d-76            [-1, 128, 4, 4]             256\n",
      "             ReLU-77            [-1, 128, 4, 4]               0\n",
      "           Conv2d-78             [-1, 32, 4, 4]          36,864\n",
      "      _DenseLayer-79             [-1, 32, 4, 4]               0\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 128, 4, 4]          32,768\n",
      "      BatchNorm2d-83            [-1, 128, 4, 4]             256\n",
      "             ReLU-84            [-1, 128, 4, 4]               0\n",
      "           Conv2d-85             [-1, 32, 4, 4]          36,864\n",
      "      _DenseLayer-86             [-1, 32, 4, 4]               0\n",
      "      BatchNorm2d-87            [-1, 288, 4, 4]             576\n",
      "             ReLU-88            [-1, 288, 4, 4]               0\n",
      "           Conv2d-89            [-1, 128, 4, 4]          36,864\n",
      "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
      "             ReLU-91            [-1, 128, 4, 4]               0\n",
      "           Conv2d-92             [-1, 32, 4, 4]          36,864\n",
      "      _DenseLayer-93             [-1, 32, 4, 4]               0\n",
      "      BatchNorm2d-94            [-1, 320, 4, 4]             640\n",
      "             ReLU-95            [-1, 320, 4, 4]               0\n",
      "           Conv2d-96            [-1, 128, 4, 4]          40,960\n",
      "      BatchNorm2d-97            [-1, 128, 4, 4]             256\n",
      "             ReLU-98            [-1, 128, 4, 4]               0\n",
      "           Conv2d-99             [-1, 32, 4, 4]          36,864\n",
      "     _DenseLayer-100             [-1, 32, 4, 4]               0\n",
      "     BatchNorm2d-101            [-1, 352, 4, 4]             704\n",
      "            ReLU-102            [-1, 352, 4, 4]               0\n",
      "          Conv2d-103            [-1, 128, 4, 4]          45,056\n",
      "     BatchNorm2d-104            [-1, 128, 4, 4]             256\n",
      "            ReLU-105            [-1, 128, 4, 4]               0\n",
      "          Conv2d-106             [-1, 32, 4, 4]          36,864\n",
      "     _DenseLayer-107             [-1, 32, 4, 4]               0\n",
      "     BatchNorm2d-108            [-1, 384, 4, 4]             768\n",
      "            ReLU-109            [-1, 384, 4, 4]               0\n",
      "          Conv2d-110            [-1, 128, 4, 4]          49,152\n",
      "     BatchNorm2d-111            [-1, 128, 4, 4]             256\n",
      "            ReLU-112            [-1, 128, 4, 4]               0\n",
      "          Conv2d-113             [-1, 32, 4, 4]          36,864\n",
      "     _DenseLayer-114             [-1, 32, 4, 4]               0\n",
      "     BatchNorm2d-115            [-1, 416, 4, 4]             832\n",
      "            ReLU-116            [-1, 416, 4, 4]               0\n",
      "          Conv2d-117            [-1, 128, 4, 4]          53,248\n",
      "     BatchNorm2d-118            [-1, 128, 4, 4]             256\n",
      "            ReLU-119            [-1, 128, 4, 4]               0\n",
      "          Conv2d-120             [-1, 32, 4, 4]          36,864\n",
      "     _DenseLayer-121             [-1, 32, 4, 4]               0\n",
      "     BatchNorm2d-122            [-1, 448, 4, 4]             896\n",
      "            ReLU-123            [-1, 448, 4, 4]               0\n",
      "          Conv2d-124            [-1, 128, 4, 4]          57,344\n",
      "     BatchNorm2d-125            [-1, 128, 4, 4]             256\n",
      "            ReLU-126            [-1, 128, 4, 4]               0\n",
      "          Conv2d-127             [-1, 32, 4, 4]          36,864\n",
      "     _DenseLayer-128             [-1, 32, 4, 4]               0\n",
      "     BatchNorm2d-129            [-1, 480, 4, 4]             960\n",
      "            ReLU-130            [-1, 480, 4, 4]               0\n",
      "          Conv2d-131            [-1, 128, 4, 4]          61,440\n",
      "     BatchNorm2d-132            [-1, 128, 4, 4]             256\n",
      "            ReLU-133            [-1, 128, 4, 4]               0\n",
      "          Conv2d-134             [-1, 32, 4, 4]          36,864\n",
      "     _DenseLayer-135             [-1, 32, 4, 4]               0\n",
      "     _DenseBlock-136            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-137            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-138            [-1, 512, 4, 4]               0\n",
      "          Conv2d-139            [-1, 256, 4, 4]         131,072\n",
      "       AvgPool2d-140            [-1, 256, 2, 2]               0\n",
      "     BatchNorm2d-141            [-1, 256, 2, 2]             512\n",
      "            ReLU-142            [-1, 256, 2, 2]               0\n",
      "          Conv2d-143            [-1, 128, 2, 2]          32,768\n",
      "     BatchNorm2d-144            [-1, 128, 2, 2]             256\n",
      "            ReLU-145            [-1, 128, 2, 2]               0\n",
      "          Conv2d-146             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-147             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-148            [-1, 288, 2, 2]             576\n",
      "            ReLU-149            [-1, 288, 2, 2]               0\n",
      "          Conv2d-150            [-1, 128, 2, 2]          36,864\n",
      "     BatchNorm2d-151            [-1, 128, 2, 2]             256\n",
      "            ReLU-152            [-1, 128, 2, 2]               0\n",
      "          Conv2d-153             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-154             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-155            [-1, 320, 2, 2]             640\n",
      "            ReLU-156            [-1, 320, 2, 2]               0\n",
      "          Conv2d-157            [-1, 128, 2, 2]          40,960\n",
      "     BatchNorm2d-158            [-1, 128, 2, 2]             256\n",
      "            ReLU-159            [-1, 128, 2, 2]               0\n",
      "          Conv2d-160             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-161             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-162            [-1, 352, 2, 2]             704\n",
      "            ReLU-163            [-1, 352, 2, 2]               0\n",
      "          Conv2d-164            [-1, 128, 2, 2]          45,056\n",
      "     BatchNorm2d-165            [-1, 128, 2, 2]             256\n",
      "            ReLU-166            [-1, 128, 2, 2]               0\n",
      "          Conv2d-167             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-168             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-169            [-1, 384, 2, 2]             768\n",
      "            ReLU-170            [-1, 384, 2, 2]               0\n",
      "          Conv2d-171            [-1, 128, 2, 2]          49,152\n",
      "     BatchNorm2d-172            [-1, 128, 2, 2]             256\n",
      "            ReLU-173            [-1, 128, 2, 2]               0\n",
      "          Conv2d-174             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-175             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-176            [-1, 416, 2, 2]             832\n",
      "            ReLU-177            [-1, 416, 2, 2]               0\n",
      "          Conv2d-178            [-1, 128, 2, 2]          53,248\n",
      "     BatchNorm2d-179            [-1, 128, 2, 2]             256\n",
      "            ReLU-180            [-1, 128, 2, 2]               0\n",
      "          Conv2d-181             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-182             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-183            [-1, 448, 2, 2]             896\n",
      "            ReLU-184            [-1, 448, 2, 2]               0\n",
      "          Conv2d-185            [-1, 128, 2, 2]          57,344\n",
      "     BatchNorm2d-186            [-1, 128, 2, 2]             256\n",
      "            ReLU-187            [-1, 128, 2, 2]               0\n",
      "          Conv2d-188             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-189             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-190            [-1, 480, 2, 2]             960\n",
      "            ReLU-191            [-1, 480, 2, 2]               0\n",
      "          Conv2d-192            [-1, 128, 2, 2]          61,440\n",
      "     BatchNorm2d-193            [-1, 128, 2, 2]             256\n",
      "            ReLU-194            [-1, 128, 2, 2]               0\n",
      "          Conv2d-195             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-196             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-197            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-198            [-1, 512, 2, 2]               0\n",
      "          Conv2d-199            [-1, 128, 2, 2]          65,536\n",
      "     BatchNorm2d-200            [-1, 128, 2, 2]             256\n",
      "            ReLU-201            [-1, 128, 2, 2]               0\n",
      "          Conv2d-202             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-203             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-204            [-1, 544, 2, 2]           1,088\n",
      "            ReLU-205            [-1, 544, 2, 2]               0\n",
      "          Conv2d-206            [-1, 128, 2, 2]          69,632\n",
      "     BatchNorm2d-207            [-1, 128, 2, 2]             256\n",
      "            ReLU-208            [-1, 128, 2, 2]               0\n",
      "          Conv2d-209             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-210             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-211            [-1, 576, 2, 2]           1,152\n",
      "            ReLU-212            [-1, 576, 2, 2]               0\n",
      "          Conv2d-213            [-1, 128, 2, 2]          73,728\n",
      "     BatchNorm2d-214            [-1, 128, 2, 2]             256\n",
      "            ReLU-215            [-1, 128, 2, 2]               0\n",
      "          Conv2d-216             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-217             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-218            [-1, 608, 2, 2]           1,216\n",
      "            ReLU-219            [-1, 608, 2, 2]               0\n",
      "          Conv2d-220            [-1, 128, 2, 2]          77,824\n",
      "     BatchNorm2d-221            [-1, 128, 2, 2]             256\n",
      "            ReLU-222            [-1, 128, 2, 2]               0\n",
      "          Conv2d-223             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-224             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-225            [-1, 640, 2, 2]           1,280\n",
      "            ReLU-226            [-1, 640, 2, 2]               0\n",
      "          Conv2d-227            [-1, 128, 2, 2]          81,920\n",
      "     BatchNorm2d-228            [-1, 128, 2, 2]             256\n",
      "            ReLU-229            [-1, 128, 2, 2]               0\n",
      "          Conv2d-230             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-231             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-232            [-1, 672, 2, 2]           1,344\n",
      "            ReLU-233            [-1, 672, 2, 2]               0\n",
      "          Conv2d-234            [-1, 128, 2, 2]          86,016\n",
      "     BatchNorm2d-235            [-1, 128, 2, 2]             256\n",
      "            ReLU-236            [-1, 128, 2, 2]               0\n",
      "          Conv2d-237             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-238             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-239            [-1, 704, 2, 2]           1,408\n",
      "            ReLU-240            [-1, 704, 2, 2]               0\n",
      "          Conv2d-241            [-1, 128, 2, 2]          90,112\n",
      "     BatchNorm2d-242            [-1, 128, 2, 2]             256\n",
      "            ReLU-243            [-1, 128, 2, 2]               0\n",
      "          Conv2d-244             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-245             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-246            [-1, 736, 2, 2]           1,472\n",
      "            ReLU-247            [-1, 736, 2, 2]               0\n",
      "          Conv2d-248            [-1, 128, 2, 2]          94,208\n",
      "     BatchNorm2d-249            [-1, 128, 2, 2]             256\n",
      "            ReLU-250            [-1, 128, 2, 2]               0\n",
      "          Conv2d-251             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-252             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-253            [-1, 768, 2, 2]           1,536\n",
      "            ReLU-254            [-1, 768, 2, 2]               0\n",
      "          Conv2d-255            [-1, 128, 2, 2]          98,304\n",
      "     BatchNorm2d-256            [-1, 128, 2, 2]             256\n",
      "            ReLU-257            [-1, 128, 2, 2]               0\n",
      "          Conv2d-258             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-259             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-260            [-1, 800, 2, 2]           1,600\n",
      "            ReLU-261            [-1, 800, 2, 2]               0\n",
      "          Conv2d-262            [-1, 128, 2, 2]         102,400\n",
      "     BatchNorm2d-263            [-1, 128, 2, 2]             256\n",
      "            ReLU-264            [-1, 128, 2, 2]               0\n",
      "          Conv2d-265             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-266             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-267            [-1, 832, 2, 2]           1,664\n",
      "            ReLU-268            [-1, 832, 2, 2]               0\n",
      "          Conv2d-269            [-1, 128, 2, 2]         106,496\n",
      "     BatchNorm2d-270            [-1, 128, 2, 2]             256\n",
      "            ReLU-271            [-1, 128, 2, 2]               0\n",
      "          Conv2d-272             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-273             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-274            [-1, 864, 2, 2]           1,728\n",
      "            ReLU-275            [-1, 864, 2, 2]               0\n",
      "          Conv2d-276            [-1, 128, 2, 2]         110,592\n",
      "     BatchNorm2d-277            [-1, 128, 2, 2]             256\n",
      "            ReLU-278            [-1, 128, 2, 2]               0\n",
      "          Conv2d-279             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-280             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-281            [-1, 896, 2, 2]           1,792\n",
      "            ReLU-282            [-1, 896, 2, 2]               0\n",
      "          Conv2d-283            [-1, 128, 2, 2]         114,688\n",
      "     BatchNorm2d-284            [-1, 128, 2, 2]             256\n",
      "            ReLU-285            [-1, 128, 2, 2]               0\n",
      "          Conv2d-286             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-287             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-288            [-1, 928, 2, 2]           1,856\n",
      "            ReLU-289            [-1, 928, 2, 2]               0\n",
      "          Conv2d-290            [-1, 128, 2, 2]         118,784\n",
      "     BatchNorm2d-291            [-1, 128, 2, 2]             256\n",
      "            ReLU-292            [-1, 128, 2, 2]               0\n",
      "          Conv2d-293             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-294             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-295            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-296            [-1, 960, 2, 2]               0\n",
      "          Conv2d-297            [-1, 128, 2, 2]         122,880\n",
      "     BatchNorm2d-298            [-1, 128, 2, 2]             256\n",
      "            ReLU-299            [-1, 128, 2, 2]               0\n",
      "          Conv2d-300             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-301             [-1, 32, 2, 2]               0\n",
      "     BatchNorm2d-302            [-1, 992, 2, 2]           1,984\n",
      "            ReLU-303            [-1, 992, 2, 2]               0\n",
      "          Conv2d-304            [-1, 128, 2, 2]         126,976\n",
      "     BatchNorm2d-305            [-1, 128, 2, 2]             256\n",
      "            ReLU-306            [-1, 128, 2, 2]               0\n",
      "          Conv2d-307             [-1, 32, 2, 2]          36,864\n",
      "     _DenseLayer-308             [-1, 32, 2, 2]               0\n",
      "     _DenseBlock-309           [-1, 1024, 2, 2]               0\n",
      "     BatchNorm2d-310           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-311           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-312            [-1, 512, 2, 2]         524,288\n",
      "       AvgPool2d-313            [-1, 512, 1, 1]               0\n",
      "     BatchNorm2d-314            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-315            [-1, 512, 1, 1]               0\n",
      "          Conv2d-316            [-1, 128, 1, 1]          65,536\n",
      "     BatchNorm2d-317            [-1, 128, 1, 1]             256\n",
      "            ReLU-318            [-1, 128, 1, 1]               0\n",
      "          Conv2d-319             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-320             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-321            [-1, 544, 1, 1]           1,088\n",
      "            ReLU-322            [-1, 544, 1, 1]               0\n",
      "          Conv2d-323            [-1, 128, 1, 1]          69,632\n",
      "     BatchNorm2d-324            [-1, 128, 1, 1]             256\n",
      "            ReLU-325            [-1, 128, 1, 1]               0\n",
      "          Conv2d-326             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-327             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-328            [-1, 576, 1, 1]           1,152\n",
      "            ReLU-329            [-1, 576, 1, 1]               0\n",
      "          Conv2d-330            [-1, 128, 1, 1]          73,728\n",
      "     BatchNorm2d-331            [-1, 128, 1, 1]             256\n",
      "            ReLU-332            [-1, 128, 1, 1]               0\n",
      "          Conv2d-333             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-334             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-335            [-1, 608, 1, 1]           1,216\n",
      "            ReLU-336            [-1, 608, 1, 1]               0\n",
      "          Conv2d-337            [-1, 128, 1, 1]          77,824\n",
      "     BatchNorm2d-338            [-1, 128, 1, 1]             256\n",
      "            ReLU-339            [-1, 128, 1, 1]               0\n",
      "          Conv2d-340             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-341             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-342            [-1, 640, 1, 1]           1,280\n",
      "            ReLU-343            [-1, 640, 1, 1]               0\n",
      "          Conv2d-344            [-1, 128, 1, 1]          81,920\n",
      "     BatchNorm2d-345            [-1, 128, 1, 1]             256\n",
      "            ReLU-346            [-1, 128, 1, 1]               0\n",
      "          Conv2d-347             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-348             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-349            [-1, 672, 1, 1]           1,344\n",
      "            ReLU-350            [-1, 672, 1, 1]               0\n",
      "          Conv2d-351            [-1, 128, 1, 1]          86,016\n",
      "     BatchNorm2d-352            [-1, 128, 1, 1]             256\n",
      "            ReLU-353            [-1, 128, 1, 1]               0\n",
      "          Conv2d-354             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-355             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-356            [-1, 704, 1, 1]           1,408\n",
      "            ReLU-357            [-1, 704, 1, 1]               0\n",
      "          Conv2d-358            [-1, 128, 1, 1]          90,112\n",
      "     BatchNorm2d-359            [-1, 128, 1, 1]             256\n",
      "            ReLU-360            [-1, 128, 1, 1]               0\n",
      "          Conv2d-361             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-362             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-363            [-1, 736, 1, 1]           1,472\n",
      "            ReLU-364            [-1, 736, 1, 1]               0\n",
      "          Conv2d-365            [-1, 128, 1, 1]          94,208\n",
      "     BatchNorm2d-366            [-1, 128, 1, 1]             256\n",
      "            ReLU-367            [-1, 128, 1, 1]               0\n",
      "          Conv2d-368             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-369             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-370            [-1, 768, 1, 1]           1,536\n",
      "            ReLU-371            [-1, 768, 1, 1]               0\n",
      "          Conv2d-372            [-1, 128, 1, 1]          98,304\n",
      "     BatchNorm2d-373            [-1, 128, 1, 1]             256\n",
      "            ReLU-374            [-1, 128, 1, 1]               0\n",
      "          Conv2d-375             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-376             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-377            [-1, 800, 1, 1]           1,600\n",
      "            ReLU-378            [-1, 800, 1, 1]               0\n",
      "          Conv2d-379            [-1, 128, 1, 1]         102,400\n",
      "     BatchNorm2d-380            [-1, 128, 1, 1]             256\n",
      "            ReLU-381            [-1, 128, 1, 1]               0\n",
      "          Conv2d-382             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-383             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-384            [-1, 832, 1, 1]           1,664\n",
      "            ReLU-385            [-1, 832, 1, 1]               0\n",
      "          Conv2d-386            [-1, 128, 1, 1]         106,496\n",
      "     BatchNorm2d-387            [-1, 128, 1, 1]             256\n",
      "            ReLU-388            [-1, 128, 1, 1]               0\n",
      "          Conv2d-389             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-390             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-391            [-1, 864, 1, 1]           1,728\n",
      "            ReLU-392            [-1, 864, 1, 1]               0\n",
      "          Conv2d-393            [-1, 128, 1, 1]         110,592\n",
      "     BatchNorm2d-394            [-1, 128, 1, 1]             256\n",
      "            ReLU-395            [-1, 128, 1, 1]               0\n",
      "          Conv2d-396             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-397             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-398            [-1, 896, 1, 1]           1,792\n",
      "            ReLU-399            [-1, 896, 1, 1]               0\n",
      "          Conv2d-400            [-1, 128, 1, 1]         114,688\n",
      "     BatchNorm2d-401            [-1, 128, 1, 1]             256\n",
      "            ReLU-402            [-1, 128, 1, 1]               0\n",
      "          Conv2d-403             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-404             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-405            [-1, 928, 1, 1]           1,856\n",
      "            ReLU-406            [-1, 928, 1, 1]               0\n",
      "          Conv2d-407            [-1, 128, 1, 1]         118,784\n",
      "     BatchNorm2d-408            [-1, 128, 1, 1]             256\n",
      "            ReLU-409            [-1, 128, 1, 1]               0\n",
      "          Conv2d-410             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-411             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-412            [-1, 960, 1, 1]           1,920\n",
      "            ReLU-413            [-1, 960, 1, 1]               0\n",
      "          Conv2d-414            [-1, 128, 1, 1]         122,880\n",
      "     BatchNorm2d-415            [-1, 128, 1, 1]             256\n",
      "            ReLU-416            [-1, 128, 1, 1]               0\n",
      "          Conv2d-417             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-418             [-1, 32, 1, 1]               0\n",
      "     BatchNorm2d-419            [-1, 992, 1, 1]           1,984\n",
      "            ReLU-420            [-1, 992, 1, 1]               0\n",
      "          Conv2d-421            [-1, 128, 1, 1]         126,976\n",
      "     BatchNorm2d-422            [-1, 128, 1, 1]             256\n",
      "            ReLU-423            [-1, 128, 1, 1]               0\n",
      "          Conv2d-424             [-1, 32, 1, 1]          36,864\n",
      "     _DenseLayer-425             [-1, 32, 1, 1]               0\n",
      "     _DenseBlock-426           [-1, 1024, 1, 1]               0\n",
      "     BatchNorm2d-427           [-1, 1024, 1, 1]           2,048\n",
      "          Linear-428                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 6,964,106\n",
      "Trainable params: 6,964,106\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.40\n",
      "Params size (MB): 26.57\n",
      "Estimated Total Size (MB): 32.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "densenet = torchvision.models.densenet121()\n",
    "densenet.classifier = nn.Linear(in_features=1024, out_features=num_classes, bias=True)\n",
    "densenet = densenet.to(device)\n",
    "summary(densenet, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 96, 8, 8]           4,704\n",
      "       LayerNorm2d-2             [-1, 96, 8, 8]             192\n",
      "            Conv2d-3             [-1, 96, 8, 8]           4,800\n",
      "           Permute-4             [-1, 8, 8, 96]               0\n",
      "         LayerNorm-5             [-1, 8, 8, 96]             192\n",
      "            Linear-6            [-1, 8, 8, 384]          37,248\n",
      "              GELU-7            [-1, 8, 8, 384]               0\n",
      "            Linear-8             [-1, 8, 8, 96]          36,960\n",
      "           Permute-9             [-1, 96, 8, 8]               0\n",
      "  StochasticDepth-10             [-1, 96, 8, 8]               0\n",
      "          CNBlock-11             [-1, 96, 8, 8]               0\n",
      "           Conv2d-12             [-1, 96, 8, 8]           4,800\n",
      "          Permute-13             [-1, 8, 8, 96]               0\n",
      "        LayerNorm-14             [-1, 8, 8, 96]             192\n",
      "           Linear-15            [-1, 8, 8, 384]          37,248\n",
      "             GELU-16            [-1, 8, 8, 384]               0\n",
      "           Linear-17             [-1, 8, 8, 96]          36,960\n",
      "          Permute-18             [-1, 96, 8, 8]               0\n",
      "  StochasticDepth-19             [-1, 96, 8, 8]               0\n",
      "          CNBlock-20             [-1, 96, 8, 8]               0\n",
      "           Conv2d-21             [-1, 96, 8, 8]           4,800\n",
      "          Permute-22             [-1, 8, 8, 96]               0\n",
      "        LayerNorm-23             [-1, 8, 8, 96]             192\n",
      "           Linear-24            [-1, 8, 8, 384]          37,248\n",
      "             GELU-25            [-1, 8, 8, 384]               0\n",
      "           Linear-26             [-1, 8, 8, 96]          36,960\n",
      "          Permute-27             [-1, 96, 8, 8]               0\n",
      "  StochasticDepth-28             [-1, 96, 8, 8]               0\n",
      "          CNBlock-29             [-1, 96, 8, 8]               0\n",
      "      LayerNorm2d-30             [-1, 96, 8, 8]             192\n",
      "           Conv2d-31            [-1, 192, 4, 4]          73,920\n",
      "           Conv2d-32            [-1, 192, 4, 4]           9,600\n",
      "          Permute-33            [-1, 4, 4, 192]               0\n",
      "        LayerNorm-34            [-1, 4, 4, 192]             384\n",
      "           Linear-35            [-1, 4, 4, 768]         148,224\n",
      "             GELU-36            [-1, 4, 4, 768]               0\n",
      "           Linear-37            [-1, 4, 4, 192]         147,648\n",
      "          Permute-38            [-1, 192, 4, 4]               0\n",
      "  StochasticDepth-39            [-1, 192, 4, 4]               0\n",
      "          CNBlock-40            [-1, 192, 4, 4]               0\n",
      "           Conv2d-41            [-1, 192, 4, 4]           9,600\n",
      "          Permute-42            [-1, 4, 4, 192]               0\n",
      "        LayerNorm-43            [-1, 4, 4, 192]             384\n",
      "           Linear-44            [-1, 4, 4, 768]         148,224\n",
      "             GELU-45            [-1, 4, 4, 768]               0\n",
      "           Linear-46            [-1, 4, 4, 192]         147,648\n",
      "          Permute-47            [-1, 192, 4, 4]               0\n",
      "  StochasticDepth-48            [-1, 192, 4, 4]               0\n",
      "          CNBlock-49            [-1, 192, 4, 4]               0\n",
      "           Conv2d-50            [-1, 192, 4, 4]           9,600\n",
      "          Permute-51            [-1, 4, 4, 192]               0\n",
      "        LayerNorm-52            [-1, 4, 4, 192]             384\n",
      "           Linear-53            [-1, 4, 4, 768]         148,224\n",
      "             GELU-54            [-1, 4, 4, 768]               0\n",
      "           Linear-55            [-1, 4, 4, 192]         147,648\n",
      "          Permute-56            [-1, 192, 4, 4]               0\n",
      "  StochasticDepth-57            [-1, 192, 4, 4]               0\n",
      "          CNBlock-58            [-1, 192, 4, 4]               0\n",
      "      LayerNorm2d-59            [-1, 192, 4, 4]             384\n",
      "           Conv2d-60            [-1, 384, 2, 2]         295,296\n",
      "           Conv2d-61            [-1, 384, 2, 2]          19,200\n",
      "          Permute-62            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-63            [-1, 2, 2, 384]             768\n",
      "           Linear-64           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-65           [-1, 2, 2, 1536]               0\n",
      "           Linear-66            [-1, 2, 2, 384]         590,208\n",
      "          Permute-67            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-68            [-1, 384, 2, 2]               0\n",
      "          CNBlock-69            [-1, 384, 2, 2]               0\n",
      "           Conv2d-70            [-1, 384, 2, 2]          19,200\n",
      "          Permute-71            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-72            [-1, 2, 2, 384]             768\n",
      "           Linear-73           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-74           [-1, 2, 2, 1536]               0\n",
      "           Linear-75            [-1, 2, 2, 384]         590,208\n",
      "          Permute-76            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-77            [-1, 384, 2, 2]               0\n",
      "          CNBlock-78            [-1, 384, 2, 2]               0\n",
      "           Conv2d-79            [-1, 384, 2, 2]          19,200\n",
      "          Permute-80            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-81            [-1, 2, 2, 384]             768\n",
      "           Linear-82           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-83           [-1, 2, 2, 1536]               0\n",
      "           Linear-84            [-1, 2, 2, 384]         590,208\n",
      "          Permute-85            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-86            [-1, 384, 2, 2]               0\n",
      "          CNBlock-87            [-1, 384, 2, 2]               0\n",
      "           Conv2d-88            [-1, 384, 2, 2]          19,200\n",
      "          Permute-89            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-90            [-1, 2, 2, 384]             768\n",
      "           Linear-91           [-1, 2, 2, 1536]         591,360\n",
      "             GELU-92           [-1, 2, 2, 1536]               0\n",
      "           Linear-93            [-1, 2, 2, 384]         590,208\n",
      "          Permute-94            [-1, 384, 2, 2]               0\n",
      "  StochasticDepth-95            [-1, 384, 2, 2]               0\n",
      "          CNBlock-96            [-1, 384, 2, 2]               0\n",
      "           Conv2d-97            [-1, 384, 2, 2]          19,200\n",
      "          Permute-98            [-1, 2, 2, 384]               0\n",
      "        LayerNorm-99            [-1, 2, 2, 384]             768\n",
      "          Linear-100           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-101           [-1, 2, 2, 1536]               0\n",
      "          Linear-102            [-1, 2, 2, 384]         590,208\n",
      "         Permute-103            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-104            [-1, 384, 2, 2]               0\n",
      "         CNBlock-105            [-1, 384, 2, 2]               0\n",
      "          Conv2d-106            [-1, 384, 2, 2]          19,200\n",
      "         Permute-107            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-108            [-1, 2, 2, 384]             768\n",
      "          Linear-109           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-110           [-1, 2, 2, 1536]               0\n",
      "          Linear-111            [-1, 2, 2, 384]         590,208\n",
      "         Permute-112            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-113            [-1, 384, 2, 2]               0\n",
      "         CNBlock-114            [-1, 384, 2, 2]               0\n",
      "          Conv2d-115            [-1, 384, 2, 2]          19,200\n",
      "         Permute-116            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-117            [-1, 2, 2, 384]             768\n",
      "          Linear-118           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-119           [-1, 2, 2, 1536]               0\n",
      "          Linear-120            [-1, 2, 2, 384]         590,208\n",
      "         Permute-121            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-122            [-1, 384, 2, 2]               0\n",
      "         CNBlock-123            [-1, 384, 2, 2]               0\n",
      "          Conv2d-124            [-1, 384, 2, 2]          19,200\n",
      "         Permute-125            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-126            [-1, 2, 2, 384]             768\n",
      "          Linear-127           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-128           [-1, 2, 2, 1536]               0\n",
      "          Linear-129            [-1, 2, 2, 384]         590,208\n",
      "         Permute-130            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-131            [-1, 384, 2, 2]               0\n",
      "         CNBlock-132            [-1, 384, 2, 2]               0\n",
      "          Conv2d-133            [-1, 384, 2, 2]          19,200\n",
      "         Permute-134            [-1, 2, 2, 384]               0\n",
      "       LayerNorm-135            [-1, 2, 2, 384]             768\n",
      "          Linear-136           [-1, 2, 2, 1536]         591,360\n",
      "            GELU-137           [-1, 2, 2, 1536]               0\n",
      "          Linear-138            [-1, 2, 2, 384]         590,208\n",
      "         Permute-139            [-1, 384, 2, 2]               0\n",
      " StochasticDepth-140            [-1, 384, 2, 2]               0\n",
      "         CNBlock-141            [-1, 384, 2, 2]               0\n",
      "     LayerNorm2d-142            [-1, 384, 2, 2]             768\n",
      "          Conv2d-143            [-1, 768, 1, 1]       1,180,416\n",
      "          Conv2d-144            [-1, 768, 1, 1]          38,400\n",
      "         Permute-145            [-1, 1, 1, 768]               0\n",
      "       LayerNorm-146            [-1, 1, 1, 768]           1,536\n",
      "          Linear-147           [-1, 1, 1, 3072]       2,362,368\n",
      "            GELU-148           [-1, 1, 1, 3072]               0\n",
      "          Linear-149            [-1, 1, 1, 768]       2,360,064\n",
      "         Permute-150            [-1, 768, 1, 1]               0\n",
      " StochasticDepth-151            [-1, 768, 1, 1]               0\n",
      "         CNBlock-152            [-1, 768, 1, 1]               0\n",
      "          Conv2d-153            [-1, 768, 1, 1]          38,400\n",
      "         Permute-154            [-1, 1, 1, 768]               0\n",
      "       LayerNorm-155            [-1, 1, 1, 768]           1,536\n",
      "          Linear-156           [-1, 1, 1, 3072]       2,362,368\n",
      "            GELU-157           [-1, 1, 1, 3072]               0\n",
      "          Linear-158            [-1, 1, 1, 768]       2,360,064\n",
      "         Permute-159            [-1, 768, 1, 1]               0\n",
      " StochasticDepth-160            [-1, 768, 1, 1]               0\n",
      "         CNBlock-161            [-1, 768, 1, 1]               0\n",
      "          Conv2d-162            [-1, 768, 1, 1]          38,400\n",
      "         Permute-163            [-1, 1, 1, 768]               0\n",
      "       LayerNorm-164            [-1, 1, 1, 768]           1,536\n",
      "          Linear-165           [-1, 1, 1, 3072]       2,362,368\n",
      "            GELU-166           [-1, 1, 1, 3072]               0\n",
      "          Linear-167            [-1, 1, 1, 768]       2,360,064\n",
      "         Permute-168            [-1, 768, 1, 1]               0\n",
      " StochasticDepth-169            [-1, 768, 1, 1]               0\n",
      "         CNBlock-170            [-1, 768, 1, 1]               0\n",
      "AdaptiveAvgPool2d-171            [-1, 768, 1, 1]               0\n",
      "     LayerNorm2d-172            [-1, 768, 1, 1]           1,536\n",
      "         Flatten-173                  [-1, 768]               0\n",
      "          Linear-174                   [-1, 10]           7,690\n",
      "================================================================\n",
      "Total params: 27,821,194\n",
      "Trainable params: 27,821,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.24\n",
      "Params size (MB): 106.13\n",
      "Estimated Total Size (MB): 111.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "convnext = torchvision.models.convnext_tiny()\n",
    "convnext.classifier[2] = nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "convnext.to(device)\n",
    "summary(convnext, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 256, 256]              84\n",
      "       BatchNorm2d-2          [-1, 3, 256, 256]               6\n",
      "              ReLU-3          [-1, 3, 256, 256]               0\n",
      "            Conv2d-4         [-1, 16, 256, 256]             448\n",
      "       BatchNorm2d-5         [-1, 16, 256, 256]              32\n",
      "              ReLU-6         [-1, 16, 256, 256]               0\n",
      "         MaxPool2d-7         [-1, 16, 128, 128]               0\n",
      "          ResBlock-8         [-1, 16, 128, 128]               0\n",
      "            Conv2d-9         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-10         [-1, 16, 128, 128]              32\n",
      "             ReLU-11         [-1, 16, 128, 128]               0\n",
      "           Conv2d-12         [-1, 32, 128, 128]           4,640\n",
      "      BatchNorm2d-13         [-1, 32, 128, 128]              64\n",
      "             ReLU-14         [-1, 32, 128, 128]               0\n",
      "        MaxPool2d-15           [-1, 32, 64, 64]               0\n",
      "         ResBlock-16           [-1, 32, 64, 64]               0\n",
      "           Conv2d-17           [-1, 32, 64, 64]           9,248\n",
      "      BatchNorm2d-18           [-1, 32, 64, 64]              64\n",
      "             ReLU-19           [-1, 32, 64, 64]               0\n",
      "           Conv2d-20           [-1, 64, 64, 64]          18,496\n",
      "      BatchNorm2d-21           [-1, 64, 64, 64]             128\n",
      "             ReLU-22           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-23           [-1, 64, 32, 32]               0\n",
      "         ResBlock-24           [-1, 64, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]          36,928\n",
      "      BatchNorm2d-26           [-1, 64, 32, 32]             128\n",
      "             ReLU-27           [-1, 64, 32, 32]               0\n",
      "           Conv2d-28          [-1, 128, 32, 32]          73,856\n",
      "      BatchNorm2d-29          [-1, 128, 32, 32]             256\n",
      "             ReLU-30          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-31          [-1, 128, 16, 16]               0\n",
      "         ResBlock-32          [-1, 128, 16, 16]               0\n",
      "           Conv2d-33          [-1, 256, 14, 14]         295,168\n",
      "        MaxPool2d-34            [-1, 256, 7, 7]               0\n",
      "          Flatten-35                [-1, 12544]               0\n",
      "           Linear-36                    [-1, 2]          25,090\n",
      "================================================================\n",
      "Total params: 466,988\n",
      "Trainable params: 466,988\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 68.07\n",
      "Params size (MB): 1.78\n",
      "Estimated Total Size (MB): 70.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sodnet = SODNet(3, 16)\n",
    "sodnet = sodnet.to(device)\n",
    "summary(sodnet, input_size=(3, 256, 256))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
