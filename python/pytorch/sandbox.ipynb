{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "from clf_funcs import fit, test, get_cifar10_loaders, get_mnist_loaders, FullyConnectedNet, SimpleConvNet\n",
    "from dcgan_funcs import fit_dcgan, get_celeba_loader, get_celeba_loader_from_memory_old, Discriminator, Generator, dcgan_weights_init, generate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import reduce\n",
    "flat_map = lambda f, xs: reduce(lambda a, b: a + b, map(f, xs), [])\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import mobilenet_v2, resnet50\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch no. 0\n",
      "processing batch no. 50\n",
      "processing batch no. 100\n",
      "processing batch no. 150\n",
      "processing batch no. 200\n",
      "processing batch no. 250\n",
      "processing batch no. 300\n",
      "processing batch no. 350\n",
      "processing batch no. 400\n",
      "processing batch no. 450\n",
      "processing batch no. 500\n"
     ]
    }
   ],
   "source": [
    "dl = get_celeba_loader(96, root='../../datasets/celeba_trunc/')\n",
    "all_batches = [batch[0] for batch in dl]\n",
    "all_batches = []\n",
    "for i, batch in enumerate(dl):\n",
    "\tif i % 50 == 0: print(f'processing batch no. {i}')\n",
    "\tall_batches.append(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected = torch.concat(all_batches, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celeba_loader_from_memory(batch_size, image_size=64, root='../../datasets/celeba'):\n",
    "\tdl = get_celeba_loader(batch_size, image_size=image_size, root=root)\n",
    "\n",
    "\t# all_batches = [batch[0] for batch in dl]\n",
    "\tcollected_batches = []\n",
    "\tfor i, batch in enumerate(dl):\n",
    "\t\tif i % 50 == 0: print(f'processing batch no. {i}')\n",
    "\t\tcollected_batches.append(batch)\n",
    "\n",
    "\treturn collected_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dl = get_celeba_loader_from_memory(96, root='../../datasets/celeba_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 3, 64, 64]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba_dl[0][0].shape, celeba_dl[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "epochs = 2\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "log_interval = 300\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'CUDA enabled: {use_cuda}')\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenet_v2()\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mobilenet_v2()\n",
    "# model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "train_dl, test_dl = get_cifar10_loaders(batch_size, test_batch_size)\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "\ttrain_history = fit(model, device, train_dl, loss_func, epoch, optimizer=opt, log_interval=log_interval, silent=False)\n",
    "\t_, accuracy = test(model, device, test_dl, loss_func, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 3\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "lr = 1e-2\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(nc, nz, ngf).to(device)\n",
    "netD = Discriminator(nc, ndf).to(device)\n",
    "netG.apply(dcgan_weights_init)\n",
    "netD.apply(dcgan_weights_init)\n",
    "\n",
    "# celeba_dl = get_celeba_loader(batch_size=batch_size, root='../../datasets/celeba_trunc/')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1.\n",
    "fake_label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "96\n",
      "torch.Size([3, 64, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \tgan_hist \u001b[39m=\u001b[39m fit_dcgan(netG, netD, device, celeba_dl, criterion, e, optimizerG, optimizerD, nz, log_interval\u001b[39m=\u001b[39;49mlog_interval)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m stat \u001b[39min\u001b[39;00m gan_hist:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \t\tgan_hist[stat] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(gan_hist[stat]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(gan_hist[stat])\n",
      "\u001b[1;32m/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m b_size \u001b[39m=\u001b[39m real_cpu\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b_size,), real_label, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m output \u001b[39m=\u001b[39m discriminator(real_cpu)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m errD_real \u001b[39m=\u001b[39m loss_func(output, label)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubdesktop/home/mihawb/dnn-lang-comparison/python/pytorch/sandbox.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m errD_real\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dnn-lang-comparison/python/pytorch/dcgan_funcs.py:179\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:416\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "for e in range(1, epochs + 1):\n",
    "\tgan_hist = fit_dcgan(netG, netD, device, celeba_dl, criterion, e, optimizerG, optimizerD, nz, log_interval=log_interval)\n",
    "\n",
    "\tfor stat in gan_hist:\n",
    "\t\tgan_hist[stat] = np.sum(gan_hist[stat]) / len(gan_hist[stat])\n",
    "\tprint(gan_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(netG, device, 2, save=True, latent_vecs_batch=fixed_noise)\n",
    "# pytorch_dcgan_results_1702854757982334094.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([64, 3, 64, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgs), imgs.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsnew = np.transpose(torchvision.utils.make_grid(imgs, padding=5, normalize=True).cpu(),(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([557, 557, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgsnew), imgsnew.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, dtype('float32'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imgsnew.numpy()), imgs.numpy().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,intermediate_channels,expansion,is_Bottleneck,stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a Bottleneck with conv 1x1->3x3->1x1 layers.\n",
    "        \n",
    "        Note:\n",
    "          1. Addition of feature maps occur at just before the final ReLU with the input feature maps\n",
    "          2. if input size is different from output, select projected mapping or else identity mapping.\n",
    "          3. if is_Bottleneck=False (3x3->3x3) are used else (1x1->3x3->1x1). Bottleneck is required for resnet-50/101/152\n",
    "        Args:\n",
    "            in_channels (int) : input channels to the Bottleneck\n",
    "            intermediate_channels (int) : number of channels to 3x3 conv \n",
    "            expansion (int) : factor by which the input #channels are increased\n",
    "            stride (int) : stride applied in the 3x3 conv. 2 for first Bottleneck of the block and 1 for remaining\n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(Bottleneck,self).__init__()\n",
    "\n",
    "        self.expansion = expansion\n",
    "        self.in_channels = in_channels\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "        self.is_Bottleneck = is_Bottleneck\n",
    "        \n",
    "        # i.e. if dim(x) == dim(F) => Identity function\n",
    "        if self.in_channels==self.intermediate_channels*self.expansion:\n",
    "            self.identity = True\n",
    "        else:\n",
    "            self.identity = False\n",
    "            projection_layer = []\n",
    "            projection_layer.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=stride, padding=0, bias=False ))\n",
    "            projection_layer.append(nn.BatchNorm2d(self.intermediate_channels*self.expansion))\n",
    "            # Only conv->BN and no ReLU\n",
    "            # projection_layer.append(nn.ReLU())\n",
    "            self.projection = nn.Sequential(*projection_layer)\n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # is_Bottleneck = True for all ResNet 50+\n",
    "        if self.is_Bottleneck:\n",
    "            # bottleneck\n",
    "            # 1x1\n",
    "            self.conv1_1x1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 1x1\n",
    "            self.conv3_1x1 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm3 = nn.BatchNorm2d( self.intermediate_channels*self.expansion )\n",
    "        \n",
    "        else:\n",
    "            # basicblock\n",
    "            # 3x3\n",
    "            self.conv1_3x3 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # input stored to be added before the final relu\n",
    "        in_x = x\n",
    "\n",
    "        if self.is_Bottleneck:\n",
    "            # conv1x1->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_1x1(x)))\n",
    "            \n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm2(self.conv2_3x3(x)))\n",
    "            \n",
    "            # conv1x1->BN\n",
    "            x = self.batchnorm3(self.conv3_1x1(x))\n",
    "        \n",
    "        else:\n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_3x3(x)))\n",
    "\n",
    "            # conv3x3->BN\n",
    "            x = self.batchnorm2(self.conv2_3x3(x))\n",
    "\n",
    "\n",
    "        # identity or projected mapping\n",
    "        if self.identity:\n",
    "            x += in_x\n",
    "        else:\n",
    "            x += self.projection(in_x)\n",
    "\n",
    "        # final relu\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Bottleneck(64*4,64,4,stride=1)\n",
    "\n",
    "def test_Bottleneck():\n",
    "    x = torch.randn(1,64,112,112)\n",
    "    model = Bottleneck(64,64,4,True,2)\n",
    "    print(model(x).shape)\n",
    "    del model\n",
    "\n",
    "test_Bottleneck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_variant,in_channels,num_classes):\n",
    "        \"\"\"\n",
    "        Creates the ResNet architecture based on the provided variant. 18/34/50/101 etc.\n",
    "        Based on the input parameters, define the channels list, repeatition list along with expansion factor(4) and stride(3/1)\n",
    "        using _make_blocks method, create a sequence of multiple Bottlenecks\n",
    "        Average Pool at the end before the FC layer \n",
    "\n",
    "        Args:\n",
    "            resnet_variant (list) : eg. [[64,128,256,512],[3,4,6,3],4,True]\n",
    "            in_channels (int) : image channels (3)\n",
    "            num_classes (int) : output #classes \n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "        super(ResNet,self).__init__()\n",
    "        self.channels_list = resnet_variant[0]\n",
    "        self.repeatition_list = resnet_variant[1]\n",
    "        self.expansion = resnet_variant[2]\n",
    "        self.is_Bottleneck = resnet_variant[3]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        # self.block1 = self._make_blocks( 64 , self.channels_list[0], self.repeatition_list[0], self.expansion, self.is_Bottleneck, stride=1 )\n",
    "        self.block1 = nn.Sequential(*[\n",
    "            Bottleneck(64,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[0],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block2 = self._make_blocks( self.channels_list[0]*self.expansion , self.channels_list[1], self.repeatition_list[1], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block2 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[0] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[1],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block3 = self._make_blocks( self.channels_list[1]*self.expansion , self.channels_list[2], self.repeatition_list[2], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block3 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[1] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[2],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        # self.block4 = self._make_blocks( self.channels_list[2]*self.expansion , self.channels_list[3], self.repeatition_list[3], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block4 = nn.Sequential(*[\n",
    "            Bottleneck(self.channels_list[2] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=2),\n",
    "            Bottleneck(self.channels_list[3] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=1),\n",
    "            Bottleneck(self.channels_list[3] * self.expansion,self.channels_list[3],self.expansion,self.is_Bottleneck,stride=1),\n",
    "        ])\n",
    "\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear( self.channels_list[3]*self.expansion , num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        x = self.average_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _make_blocks(self,in_channels,intermediate_channels,num_repeat, expansion, is_Bottleneck, stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels : #channels of the Bottleneck input\n",
    "            intermediate_channels : #channels of the 3x3 in the Bottleneck\n",
    "            num_repeat : #Bottlenecks in the block\n",
    "            expansion : factor by which intermediate_channels are multiplied to create the output channels\n",
    "            is_Bottleneck : status if Bottleneck in required\n",
    "            stride : stride to be used in the first Bottleneck conv 3x3\n",
    "\n",
    "        Attributes:\n",
    "            Sequence of Bottleneck layers\n",
    "\n",
    "        \"\"\"\n",
    "        layers = [] \n",
    "\n",
    "        layers.append(Bottleneck(in_channels,intermediate_channels,expansion,is_Bottleneck,stride=stride))\n",
    "        for num in range(1,num_repeat):\n",
    "            layers.append(Bottleneck(intermediate_channels*expansion,intermediate_channels,expansion,is_Bottleneck,stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test_ResNet(params):\n",
    "    model = ResNet( params , in_channels=3, num_classes=1000)\n",
    "    x = torch.randn(1,3,224,224)\n",
    "    output = model(x)\n",
    "    print(output.shape)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_parameters={}\n",
    "model_parameters['resnet18'] = ([64,128,256,512],[2,2,2,2],1,False)\n",
    "model_parameters['resnet34'] = ([64,128,256,512],[3,4,6,3],1,False)\n",
    "model_parameters['resnet50'] = ([64,128,256,512],[3,4,6,3],4,True)\n",
    "model_parameters['resnet101'] = ([64,128,256,512],[3,4,23,3],4,True)\n",
    "model_parameters['resnet152'] = ([64,128,256,512],[3,8,36,3],4,True)\n",
    "\n",
    "architecture = 'resnet50'\n",
    "model = test_ResNet(model_parameters[architecture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 800]         628,000\n",
      "            Linear-2                   [-1, 10]           8,010\n",
      "================================================================\n",
      "Total params: 636,010\n",
      "Trainable params: 636,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 2.43\n",
      "Estimated Total Size (MB): 2.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class FCNet(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, layers=[784, 800, 10]):\n",
    "\t\tsuper(FCNet, self).__init__()\n",
    "\t\tself.layers = nn.ModuleList([nn.Linear(a, b) for a, b in zip(layers[:-1], layers[1:])])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor layer in self.layers[:-1]:\n",
    "\t\t\tx = F.relu(layer(x))\n",
    "\t\tx = self.layers[-1](x)\n",
    "\t\treturn F.log_softmax(x, dim=1)\n",
    "\n",
    "train_dl, _, _ = get_mnist_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "fcnet = FCNet()\n",
    "fcnet = fcnet.to(device)\n",
    "summary(fcnet, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]          12,832\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                  [-1, 500]         784,500\n",
      "            Linear-8                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 802,758\n",
      "Trainable params: 802,758\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.33\n",
      "Params size (MB): 3.06\n",
      "Estimated Total Size (MB): 3.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SCVNet(nn.Module):\n",
    "\n",
    "\tdef __init__(self, num_classes=10):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv1 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(1, 16, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(),                                       \n",
    "\t\t\tnn.MaxPool2d(2)\n",
    "\t\t)\n",
    "\t\tself.conv2 = nn.Sequential(         \n",
    "\t\t\tnn.Conv2d(16, 32, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),  \n",
    "\t\t)\n",
    "\t\tself.dense = nn.Linear(32 * 7 * 7, 500)\n",
    "\t\tself.classifier = nn.Linear(500, num_classes)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\tx = F.relu(self.dense(x))\n",
    "\t\treturn F.log_softmax(self.classifier(x), dim=1)\n",
    "\t\n",
    "train_dl, _, _ = get_mnist_loaders(128, flatten=False)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "print(input_size)\n",
    "\n",
    "scvnet = SCVNet()\n",
    "scvnet = scvnet.to(device)\n",
    "summary(scvnet, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 512, 4, 4]         819,200\n",
      "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
      "              ReLU-3            [-1, 512, 4, 4]               0\n",
      "   ConvTranspose2d-4            [-1, 256, 8, 8]       2,097,152\n",
      "       BatchNorm2d-5            [-1, 256, 8, 8]             512\n",
      "              ReLU-6            [-1, 256, 8, 8]               0\n",
      "   ConvTranspose2d-7          [-1, 128, 16, 16]         524,288\n",
      "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
      "              ReLU-9          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-10           [-1, 64, 32, 32]         131,072\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-13            [-1, 3, 64, 64]           3,072\n",
      "             Tanh-14            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 3,576,704\n",
      "Trainable params: 3,576,704\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 13.64\n",
      "Estimated Total Size (MB): 16.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(3, 100, 64)\n",
    "gen = gen.to(device)\n",
    "summary(gen, (100, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           3,072\n",
      "         LeakyReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]         131,072\n",
      "       BatchNorm2d-4          [-1, 128, 16, 16]             256\n",
      "         LeakyReLU-5          [-1, 128, 16, 16]               0\n",
      "            Conv2d-6            [-1, 256, 8, 8]         524,288\n",
      "       BatchNorm2d-7            [-1, 256, 8, 8]             512\n",
      "         LeakyReLU-8            [-1, 256, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 4, 4]       2,097,152\n",
      "      BatchNorm2d-10            [-1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-11            [-1, 512, 4, 4]               0\n",
      "           Conv2d-12              [-1, 1, 1, 1]           8,192\n",
      "          Sigmoid-13              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,765,568\n",
      "Trainable params: 2,765,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 2.31\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 12.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator(3, 64)\n",
    "disc = disc.to(device)\n",
    "summary(disc, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_noise = torch.randn(32, 100, 1, 1, device=device)\n",
    "print(fixed_noise.shape)\n",
    "gen(fixed_noise).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [-1, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
      "             ReLU-93            [-1, 256, 2, 2]               0\n",
      "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
      "             ReLU-96            [-1, 256, 2, 2]               0\n",
      "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
      "            ReLU-103            [-1, 256, 2, 2]               0\n",
      "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
      "            ReLU-106            [-1, 256, 2, 2]               0\n",
      "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
      "            ReLU-113            [-1, 256, 2, 2]               0\n",
      "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
      "            ReLU-116            [-1, 256, 2, 2]               0\n",
      "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
      "            ReLU-123            [-1, 256, 2, 2]               0\n",
      "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
      "            ReLU-126            [-1, 256, 2, 2]               0\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-143            [-1, 512, 2, 2]               0\n",
      "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-146            [-1, 512, 1, 1]               0\n",
      "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-158            [-1, 512, 1, 1]               0\n",
      "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-165            [-1, 512, 1, 1]               0\n",
      "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-168            [-1, 512, 1, 1]               0\n",
      "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.86\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 95.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rn = resnet50()\n",
    "rn.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "rn = rn.to(device)\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(rn, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "       Bottleneck-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "       Bottleneck-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]         131,072\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [-1, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [-1, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-91            [-1, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-92            [-1, 256, 2, 2]             512\n",
      "             ReLU-93            [-1, 256, 2, 2]               0\n",
      "           Conv2d-94            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 2, 2]             512\n",
      "             ReLU-96            [-1, 256, 2, 2]               0\n",
      "           Conv2d-97           [-1, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-101            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102            [-1, 256, 2, 2]             512\n",
      "            ReLU-103            [-1, 256, 2, 2]               0\n",
      "          Conv2d-104            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105            [-1, 256, 2, 2]             512\n",
      "            ReLU-106            [-1, 256, 2, 2]               0\n",
      "          Conv2d-107           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-111            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-112            [-1, 256, 2, 2]             512\n",
      "            ReLU-113            [-1, 256, 2, 2]               0\n",
      "          Conv2d-114            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-115            [-1, 256, 2, 2]             512\n",
      "            ReLU-116            [-1, 256, 2, 2]               0\n",
      "          Conv2d-117           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-121            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-122            [-1, 256, 2, 2]             512\n",
      "            ReLU-123            [-1, 256, 2, 2]               0\n",
      "          Conv2d-124            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-125            [-1, 256, 2, 2]             512\n",
      "            ReLU-126            [-1, 256, 2, 2]               0\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-143            [-1, 512, 2, 2]               0\n",
      "          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-146            [-1, 512, 1, 1]               0\n",
      "          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-158            [-1, 512, 1, 1]               0\n",
      "          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-165            [-1, 512, 1, 1]               0\n",
      "          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-168            [-1, 512, 1, 1]               0\n",
      "          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [-1, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,522\n",
      "Trainable params: 23,528,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.86\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 95.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rn_native = ResNet(model_parameters['resnet50'], in_channels=3, num_classes=10)\n",
    "rn_native = rn_native.to(device)\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(rn_native, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-2           [-1, 32, 16, 16]              64\n",
      "             ReLU6-3           [-1, 32, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-5           [-1, 32, 16, 16]              64\n",
      "             ReLU6-6           [-1, 32, 16, 16]               0\n",
      "            Conv2d-7           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-8           [-1, 16, 16, 16]              32\n",
      "  InvertedResidual-9           [-1, 16, 16, 16]               0\n",
      "           Conv2d-10           [-1, 96, 16, 16]           1,536\n",
      "      BatchNorm2d-11           [-1, 96, 16, 16]             192\n",
      "            ReLU6-12           [-1, 96, 16, 16]               0\n",
      "           Conv2d-13             [-1, 96, 8, 8]             864\n",
      "      BatchNorm2d-14             [-1, 96, 8, 8]             192\n",
      "            ReLU6-15             [-1, 96, 8, 8]               0\n",
      "           Conv2d-16             [-1, 24, 8, 8]           2,304\n",
      "      BatchNorm2d-17             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-18             [-1, 24, 8, 8]               0\n",
      "           Conv2d-19            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-20            [-1, 144, 8, 8]             288\n",
      "            ReLU6-21            [-1, 144, 8, 8]               0\n",
      "           Conv2d-22            [-1, 144, 8, 8]           1,296\n",
      "      BatchNorm2d-23            [-1, 144, 8, 8]             288\n",
      "            ReLU6-24            [-1, 144, 8, 8]               0\n",
      "           Conv2d-25             [-1, 24, 8, 8]           3,456\n",
      "      BatchNorm2d-26             [-1, 24, 8, 8]              48\n",
      " InvertedResidual-27             [-1, 24, 8, 8]               0\n",
      "           Conv2d-28            [-1, 144, 8, 8]           3,456\n",
      "      BatchNorm2d-29            [-1, 144, 8, 8]             288\n",
      "            ReLU6-30            [-1, 144, 8, 8]               0\n",
      "           Conv2d-31            [-1, 144, 4, 4]           1,296\n",
      "      BatchNorm2d-32            [-1, 144, 4, 4]             288\n",
      "            ReLU6-33            [-1, 144, 4, 4]               0\n",
      "           Conv2d-34             [-1, 32, 4, 4]           4,608\n",
      "      BatchNorm2d-35             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-36             [-1, 32, 4, 4]               0\n",
      "           Conv2d-37            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-38            [-1, 192, 4, 4]             384\n",
      "            ReLU6-39            [-1, 192, 4, 4]               0\n",
      "           Conv2d-40            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-41            [-1, 192, 4, 4]             384\n",
      "            ReLU6-42            [-1, 192, 4, 4]               0\n",
      "           Conv2d-43             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-44             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-45             [-1, 32, 4, 4]               0\n",
      "           Conv2d-46            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-47            [-1, 192, 4, 4]             384\n",
      "            ReLU6-48            [-1, 192, 4, 4]               0\n",
      "           Conv2d-49            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-50            [-1, 192, 4, 4]             384\n",
      "            ReLU6-51            [-1, 192, 4, 4]               0\n",
      "           Conv2d-52             [-1, 32, 4, 4]           6,144\n",
      "      BatchNorm2d-53             [-1, 32, 4, 4]              64\n",
      " InvertedResidual-54             [-1, 32, 4, 4]               0\n",
      "           Conv2d-55            [-1, 192, 4, 4]           6,144\n",
      "      BatchNorm2d-56            [-1, 192, 4, 4]             384\n",
      "            ReLU6-57            [-1, 192, 4, 4]               0\n",
      "           Conv2d-58            [-1, 192, 2, 2]           1,728\n",
      "      BatchNorm2d-59            [-1, 192, 2, 2]             384\n",
      "            ReLU6-60            [-1, 192, 2, 2]               0\n",
      "           Conv2d-61             [-1, 64, 2, 2]          12,288\n",
      "      BatchNorm2d-62             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-63             [-1, 64, 2, 2]               0\n",
      "           Conv2d-64            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-65            [-1, 384, 2, 2]             768\n",
      "            ReLU6-66            [-1, 384, 2, 2]               0\n",
      "           Conv2d-67            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-68            [-1, 384, 2, 2]             768\n",
      "            ReLU6-69            [-1, 384, 2, 2]               0\n",
      "           Conv2d-70             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-71             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-72             [-1, 64, 2, 2]               0\n",
      "           Conv2d-73            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-74            [-1, 384, 2, 2]             768\n",
      "            ReLU6-75            [-1, 384, 2, 2]               0\n",
      "           Conv2d-76            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-77            [-1, 384, 2, 2]             768\n",
      "            ReLU6-78            [-1, 384, 2, 2]               0\n",
      "           Conv2d-79             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-80             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-81             [-1, 64, 2, 2]               0\n",
      "           Conv2d-82            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-83            [-1, 384, 2, 2]             768\n",
      "            ReLU6-84            [-1, 384, 2, 2]               0\n",
      "           Conv2d-85            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-86            [-1, 384, 2, 2]             768\n",
      "            ReLU6-87            [-1, 384, 2, 2]               0\n",
      "           Conv2d-88             [-1, 64, 2, 2]          24,576\n",
      "      BatchNorm2d-89             [-1, 64, 2, 2]             128\n",
      " InvertedResidual-90             [-1, 64, 2, 2]               0\n",
      "           Conv2d-91            [-1, 384, 2, 2]          24,576\n",
      "      BatchNorm2d-92            [-1, 384, 2, 2]             768\n",
      "            ReLU6-93            [-1, 384, 2, 2]               0\n",
      "           Conv2d-94            [-1, 384, 2, 2]           3,456\n",
      "      BatchNorm2d-95            [-1, 384, 2, 2]             768\n",
      "            ReLU6-96            [-1, 384, 2, 2]               0\n",
      "           Conv2d-97             [-1, 96, 2, 2]          36,864\n",
      "      BatchNorm2d-98             [-1, 96, 2, 2]             192\n",
      " InvertedResidual-99             [-1, 96, 2, 2]               0\n",
      "          Conv2d-100            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-101            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-102            [-1, 576, 2, 2]               0\n",
      "          Conv2d-103            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-104            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-105            [-1, 576, 2, 2]               0\n",
      "          Conv2d-106             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-107             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-108             [-1, 96, 2, 2]               0\n",
      "          Conv2d-109            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-110            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-111            [-1, 576, 2, 2]               0\n",
      "          Conv2d-112            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-113            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-114            [-1, 576, 2, 2]               0\n",
      "          Conv2d-115             [-1, 96, 2, 2]          55,296\n",
      "     BatchNorm2d-116             [-1, 96, 2, 2]             192\n",
      "InvertedResidual-117             [-1, 96, 2, 2]               0\n",
      "          Conv2d-118            [-1, 576, 2, 2]          55,296\n",
      "     BatchNorm2d-119            [-1, 576, 2, 2]           1,152\n",
      "           ReLU6-120            [-1, 576, 2, 2]               0\n",
      "          Conv2d-121            [-1, 576, 1, 1]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 1, 1]           1,152\n",
      "           ReLU6-123            [-1, 576, 1, 1]               0\n",
      "          Conv2d-124            [-1, 160, 1, 1]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-126            [-1, 160, 1, 1]               0\n",
      "          Conv2d-127            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-129            [-1, 960, 1, 1]               0\n",
      "          Conv2d-130            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-132            [-1, 960, 1, 1]               0\n",
      "          Conv2d-133            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-135            [-1, 160, 1, 1]               0\n",
      "          Conv2d-136            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-138            [-1, 960, 1, 1]               0\n",
      "          Conv2d-139            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-141            [-1, 960, 1, 1]               0\n",
      "          Conv2d-142            [-1, 160, 1, 1]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 1, 1]             320\n",
      "InvertedResidual-144            [-1, 160, 1, 1]               0\n",
      "          Conv2d-145            [-1, 960, 1, 1]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-147            [-1, 960, 1, 1]               0\n",
      "          Conv2d-148            [-1, 960, 1, 1]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 1, 1]           1,920\n",
      "           ReLU6-150            [-1, 960, 1, 1]               0\n",
      "          Conv2d-151            [-1, 320, 1, 1]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 1, 1]             640\n",
      "InvertedResidual-153            [-1, 320, 1, 1]               0\n",
      "          Conv2d-154           [-1, 1280, 1, 1]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 1, 1]           2,560\n",
      "           ReLU6-156           [-1, 1280, 1, 1]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                   [-1, 10]          12,810\n",
      "================================================================\n",
      "Total params: 2,236,682\n",
      "Trainable params: 2,236,682\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.13\n",
      "Params size (MB): 8.53\n",
      "Estimated Total Size (MB): 11.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = mobilenet_v2()\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dl, _ = get_cifar10_loaders(128)\n",
    "input_size = next(iter(train_dl))[0].shape[1:]\n",
    "\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 3, 64, 64]), 2111)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeba = get_celeba_loader(batch_size=96)\n",
    "next(iter(celeba))[0].shape, len(celeba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(3, 100, 64)\n",
    "gen = gen.to(device)\n",
    "disc = Discriminator(3, 64)\n",
    "disc = disc.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
