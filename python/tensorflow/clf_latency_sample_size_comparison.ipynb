{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 01:17:06.958789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 01:17:33.148693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:33.511485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:33.512080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "from clf_funcs import setup, PerfCounterCallback, env_builder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import trange\n",
    "\n",
    "setup()\n",
    "\n",
    "INDEX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry = {\n",
    "\t'framework': [],\n",
    "\t'model_name': [],\n",
    "\t'rep': [],\n",
    "\t'batch_size': [],\n",
    "\t'elapsed_time': []\n",
    "}\n",
    "\n",
    "warmup_steps = 100\n",
    "repetitions = 50\n",
    "perf_callback = PerfCounterCallback(None, [])\n",
    "\n",
    "config = {\n",
    "\t'batch_size': 1,\n",
    "\t'test_batch_size': 1,\n",
    "\t'inputs': tf.keras.layers.Input(shape=(32,32,3)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 01:17:34.062456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:34.063054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:34.063657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:37.939092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:37.939412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:37.939659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-20 01:17:37.939919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1500 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "Warmup for MobileNet-v2 (batch of 1):   0%|          | 0/100 [00:00<?, ?it/s]2025-01-20 01:17:52.767214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "Warmup for MobileNet-v2 (batch of 1): 100%|██████████| 100/100 [00:23<00:00,  4.18it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 1): 100%|██████████| 50/50 [00:02<00:00, 22.21it/s]\n",
      "Warmup for MobileNet-v2 (batch of 1): 100%|██████████| 100/100 [00:04<00:00, 24.58it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 1): 100%|██████████| 50/50 [00:02<00:00, 24.54it/s]\n",
      "Warmup for MobileNet-v2 (batch of 1): 100%|██████████| 100/100 [00:01<00:00, 61.01it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 1): 100%|██████████| 50/50 [00:00<00:00, 124.60it/s]\n",
      "Warmup for MobileNet-v2 (batch of 1): 100%|██████████| 100/100 [00:06<00:00, 16.66it/s]\n",
      "__call__ for MobileNet-v2 (batch of 1): 100%|██████████| 50/50 [00:02<00:00, 17.03it/s]\n",
      "Warmup for MobileNet-v2 (batch of 16): 100%|██████████| 100/100 [00:06<00:00, 16.00it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 16): 100%|██████████| 50/50 [00:02<00:00, 21.69it/s]\n",
      "Warmup for MobileNet-v2 (batch of 16): 100%|██████████| 100/100 [00:04<00:00, 23.82it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 16): 100%|██████████| 50/50 [00:02<00:00, 23.78it/s]\n",
      "Warmup for MobileNet-v2 (batch of 16): 100%|██████████| 100/100 [00:01<00:00, 59.32it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 16): 100%|██████████| 50/50 [00:00<00:00, 110.50it/s]\n",
      "Warmup for MobileNet-v2 (batch of 16): 100%|██████████| 100/100 [00:05<00:00, 16.80it/s]\n",
      "__call__ for MobileNet-v2 (batch of 16): 100%|██████████| 50/50 [00:02<00:00, 16.79it/s]\n",
      "Warmup for MobileNet-v2 (batch of 32): 100%|██████████| 100/100 [00:05<00:00, 17.02it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 32): 100%|██████████| 50/50 [00:02<00:00, 22.64it/s]\n",
      "Warmup for MobileNet-v2 (batch of 32): 100%|██████████| 100/100 [00:04<00:00, 20.60it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 32): 100%|██████████| 50/50 [00:02<00:00, 22.35it/s]\n",
      "Warmup for MobileNet-v2 (batch of 32): 100%|██████████| 100/100 [00:01<00:00, 52.23it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 32): 100%|██████████| 50/50 [00:00<00:00, 86.56it/s]\n",
      "Warmup for MobileNet-v2 (batch of 32): 100%|██████████| 100/100 [00:05<00:00, 16.89it/s]\n",
      "__call__ for MobileNet-v2 (batch of 32): 100%|██████████| 50/50 [00:02<00:00, 16.95it/s]\n",
      "Warmup for MobileNet-v2 (batch of 64): 100%|██████████| 100/100 [00:05<00:00, 16.98it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 64): 100%|██████████| 50/50 [00:02<00:00, 19.53it/s]\n",
      "Warmup for MobileNet-v2 (batch of 64): 100%|██████████| 100/100 [00:06<00:00, 14.52it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 64): 100%|██████████| 50/50 [00:02<00:00, 19.28it/s]\n",
      "Warmup for MobileNet-v2 (batch of 64): 100%|██████████| 100/100 [00:02<00:00, 41.33it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 64): 100%|██████████| 50/50 [00:00<00:00, 61.62it/s]\n",
      "Warmup for MobileNet-v2 (batch of 64): 100%|██████████| 100/100 [00:05<00:00, 16.90it/s]\n",
      "__call__ for MobileNet-v2 (batch of 64): 100%|██████████| 50/50 [00:02<00:00, 16.90it/s]\n",
      "Warmup for MobileNet-v2 (batch of 96): 100%|██████████| 100/100 [00:06<00:00, 15.52it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 96): 100%|██████████| 50/50 [00:02<00:00, 17.42it/s]\n",
      "Warmup for MobileNet-v2 (batch of 96): 100%|██████████| 100/100 [00:07<00:00, 13.75it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 96): 100%|██████████| 50/50 [00:02<00:00, 18.63it/s]\n",
      "Warmup for MobileNet-v2 (batch of 96): 100%|██████████| 100/100 [00:02<00:00, 35.46it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 96): 100%|██████████| 50/50 [00:01<00:00, 48.35it/s]\n",
      "Warmup for MobileNet-v2 (batch of 96): 100%|██████████| 100/100 [00:05<00:00, 16.76it/s]\n",
      "__call__ for MobileNet-v2 (batch of 96): 100%|██████████| 50/50 [00:02<00:00, 16.79it/s]\n",
      "Warmup for MobileNet-v2 (batch of 128): 100%|██████████| 100/100 [00:07<00:00, 14.00it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 128): 100%|██████████| 50/50 [00:03<00:00, 15.71it/s]\n",
      "Warmup for MobileNet-v2 (batch of 128): 100%|██████████| 100/100 [00:07<00:00, 13.48it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 128): 100%|██████████| 50/50 [00:03<00:00, 15.42it/s]\n",
      "Warmup for MobileNet-v2 (batch of 128): 100%|██████████| 100/100 [00:03<00:00, 30.62it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 128): 100%|██████████| 50/50 [00:01<00:00, 39.58it/s]\n",
      "Warmup for MobileNet-v2 (batch of 128): 100%|██████████| 100/100 [00:05<00:00, 16.87it/s]\n",
      "__call__ for MobileNet-v2 (batch of 128): 100%|██████████| 50/50 [00:02<00:00, 16.86it/s]\n",
      "Warmup for MobileNet-v2 (batch of 192): 100%|██████████| 100/100 [00:08<00:00, 11.90it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 192): 100%|██████████| 50/50 [00:03<00:00, 13.08it/s]\n",
      "Warmup for MobileNet-v2 (batch of 192): 100%|██████████| 100/100 [00:08<00:00, 12.00it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 192): 100%|██████████| 50/50 [00:03<00:00, 14.94it/s]\n",
      "Warmup for MobileNet-v2 (batch of 192): 100%|██████████| 100/100 [00:04<00:00, 22.19it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 192): 100%|██████████| 50/50 [00:01<00:00, 29.15it/s]\n",
      "Warmup for MobileNet-v2 (batch of 192): 100%|██████████| 100/100 [00:05<00:00, 16.90it/s]\n",
      "__call__ for MobileNet-v2 (batch of 192): 100%|██████████| 50/50 [00:02<00:00, 16.76it/s]\n",
      "Warmup for MobileNet-v2 (batch of 256): 100%|██████████| 100/100 [00:09<00:00, 10.23it/s]\n",
      "Predict without bsize for MobileNet-v2 (batch of 256): 100%|██████████| 50/50 [00:04<00:00, 10.87it/s]\n",
      "Warmup for MobileNet-v2 (batch of 256): 100%|██████████| 100/100 [00:09<00:00, 10.76it/s]\n",
      "Predict with bsize for MobileNet-v2 (batch of 256): 100%|██████████| 50/50 [00:03<00:00, 13.19it/s]\n",
      "Warmup for MobileNet-v2 (batch of 256): 100%|██████████| 100/100 [00:05<00:00, 19.72it/s]\n",
      "Predict on batch for MobileNet-v2 (batch of 256): 100%|██████████| 50/50 [00:02<00:00, 23.17it/s]\n",
      "Warmup for MobileNet-v2 (batch of 256): 100%|██████████| 100/100 [00:05<00:00, 16.92it/s]\n",
      "__call__ for MobileNet-v2 (batch of 256): 100%|██████████| 50/50 [00:02<00:00, 16.71it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 1):   0%|          | 0/100 [00:00<?, ?it/s]2025-01-20 01:22:37.396211: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x411af700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-01-20 01:22:37.396250: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1050, Compute Capability 6.1\n",
      "2025-01-20 01:22:39.102741: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Warmup for ConvNeXt-Tiny (batch of 1): 100%|██████████| 100/100 [00:10<00:00,  9.60it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 1): 100%|██████████| 50/50 [00:02<00:00, 18.13it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 1): 100%|██████████| 100/100 [00:05<00:00, 19.67it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 1): 100%|██████████| 50/50 [00:02<00:00, 19.59it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 1): 100%|██████████| 100/100 [00:02<00:00, 46.03it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 1): 100%|██████████| 50/50 [00:00<00:00, 94.69it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 1): 100%|██████████| 100/100 [00:11<00:00,  8.75it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 1): 100%|██████████| 50/50 [00:05<00:00,  9.49it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 16): 100%|██████████| 100/100 [00:09<00:00, 10.75it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 16): 100%|██████████| 50/50 [00:03<00:00, 16.44it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 16): 100%|██████████| 100/100 [00:05<00:00, 17.60it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 16): 100%|██████████| 50/50 [00:02<00:00, 17.55it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 16): 100%|██████████| 100/100 [00:02<00:00, 39.59it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 16): 100%|██████████| 50/50 [00:00<00:00, 71.08it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 16): 100%|██████████| 100/100 [00:11<00:00,  8.80it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 16): 100%|██████████| 50/50 [00:05<00:00,  9.67it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 32): 100%|██████████| 100/100 [00:07<00:00, 12.90it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 32): 100%|██████████| 50/50 [00:02<00:00, 18.74it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 32): 100%|██████████| 100/100 [00:05<00:00, 18.55it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 32): 100%|██████████| 50/50 [00:02<00:00, 18.56it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 32): 100%|██████████| 100/100 [00:03<00:00, 33.15it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 32): 100%|██████████| 50/50 [00:00<00:00, 53.56it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 32): 100%|██████████| 100/100 [00:11<00:00,  8.81it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 32): 100%|██████████| 50/50 [00:05<00:00,  9.58it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 64): 100%|██████████| 100/100 [00:09<00:00, 10.42it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 64): 100%|██████████| 50/50 [00:03<00:00, 15.08it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 64): 100%|██████████| 100/100 [00:10<00:00,  9.39it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 64): 100%|██████████| 50/50 [00:03<00:00, 13.29it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 64): 100%|██████████| 100/100 [00:04<00:00, 23.76it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 64): 100%|██████████| 50/50 [00:01<00:00, 32.44it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 64): 100%|██████████| 100/100 [00:11<00:00,  8.84it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 64): 100%|██████████| 50/50 [00:05<00:00,  9.58it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 96): 100%|██████████| 100/100 [00:10<00:00,  9.25it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 96): 100%|██████████| 50/50 [00:04<00:00, 12.21it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 96): 100%|██████████| 100/100 [00:12<00:00,  8.19it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 96): 100%|██████████| 50/50 [00:04<00:00, 11.04it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 96): 100%|██████████| 100/100 [00:05<00:00, 17.69it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 96): 100%|██████████| 50/50 [00:02<00:00, 22.64it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 96): 100%|██████████| 100/100 [00:12<00:00,  8.04it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 96): 100%|██████████| 50/50 [00:05<00:00,  8.37it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 128): 100%|██████████| 100/100 [00:13<00:00,  7.50it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 128): 100%|██████████| 50/50 [00:05<00:00,  9.59it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 128): 100%|██████████| 100/100 [00:14<00:00,  6.83it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 128): 100%|██████████| 50/50 [00:05<00:00,  8.63it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 128): 100%|██████████| 100/100 [00:06<00:00, 14.94it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 128): 100%|██████████| 50/50 [00:02<00:00, 17.99it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 128): 100%|██████████| 100/100 [00:11<00:00,  8.79it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 128): 100%|██████████| 50/50 [00:05<00:00,  9.52it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 192): 100%|██████████| 100/100 [00:16<00:00,  6.03it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 192): 100%|██████████| 50/50 [00:06<00:00,  7.21it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 192): 100%|██████████| 100/100 [00:18<00:00,  5.44it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 192): 100%|██████████| 50/50 [00:07<00:00,  6.81it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 192): 100%|██████████| 100/100 [00:09<00:00, 11.05it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 192): 100%|██████████| 50/50 [00:03<00:00, 12.65it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 192): 100%|██████████| 100/100 [00:11<00:00,  8.69it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 192): 100%|██████████| 50/50 [00:05<00:00,  9.53it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 256): 100%|██████████| 100/100 [00:18<00:00,  5.37it/s]\n",
      "Predict without bsize for ConvNeXt-Tiny (batch of 256): 100%|██████████| 50/50 [00:08<00:00,  6.03it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 256): 100%|██████████| 100/100 [00:21<00:00,  4.57it/s]\n",
      "Predict with bsize for ConvNeXt-Tiny (batch of 256): 100%|██████████| 50/50 [00:08<00:00,  5.84it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 256): 100%|██████████| 100/100 [00:11<00:00,  8.79it/s]\n",
      "Predict on batch for ConvNeXt-Tiny (batch of 256): 100%|██████████| 50/50 [00:05<00:00,  9.76it/s]\n",
      "Warmup for ConvNeXt-Tiny (batch of 256): 100%|██████████| 100/100 [00:11<00:00,  8.78it/s]\n",
      "__call__ for ConvNeXt-Tiny (batch of 256): 100%|██████████| 50/50 [00:05<00:00,  9.60it/s]\n"
     ]
    }
   ],
   "source": [
    "second_run = True\n",
    "\n",
    "if second_run:\n",
    "\tmodels = ['MobileNet-v2', 'ConvNeXt-Tiny']\n",
    "else:\n",
    "\tmodels = ['FullyConnectedNet', 'SimpleConvNet', 'ResNet-50', 'DenseNet-121']\n",
    "\n",
    "for model_name in models:\n",
    "\tfor batch_size in [1, 16, 32, 64, 96, 128, 192, 256]:\n",
    "\t\tconfig[\"batch_size\"] = batch_size\n",
    "\t\tmodel, ds, _ = env_builder(model_name, config)\n",
    "\n",
    "\t\tif isinstance(ds, tuple):\n",
    "\t\t\tsample = ds[0][:batch_size]\n",
    "\t\t\tsample = tf.convert_to_tensor(sample, dtype=tf.float32)\n",
    "\t\telse:\n",
    "\t\t\tsample = next(iter(ds))[0]\n",
    "\n",
    "\t\t# model.predict(batch_size=None) =================================================\n",
    "\t\tfor i in trange(warmup_steps, desc=f\"Warmup for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\t_ = model.predict(sample, verbose=0, batch_size=None)\n",
    "\n",
    "\t\tperf_callback.latency_ref.clear()\n",
    "\t\tfor i in trange(repetitions, desc=f\"Predict without bsize for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\t_ = model.predict(sample, verbose=0, callbacks=[perf_callback], batch_size=None)\n",
    "\n",
    "\t\t\ttelemetry['framework'].append(\"TF (batch_size=None)\")\n",
    "\t\t\ttelemetry['model_name'].append(model_name)\n",
    "\t\t\ttelemetry['rep'].append(i)\n",
    "\t\t\ttelemetry['batch_size'].append(batch_size)\n",
    "\t\ttelemetry['elapsed_time'].extend(perf_callback.latency_ref)\n",
    "\n",
    "\t\t# model.predict(batch_size=batch_size) ===========================================\n",
    "\t\tfor i in trange(warmup_steps, desc=f\"Warmup for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\t_ = model.predict(sample, verbose=0, batch_size=batch_size)\n",
    "\n",
    "\t\tperf_callback.latency_ref.clear()\n",
    "\t\tfor i in trange(repetitions, desc=f\"Predict with bsize for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\t_ = model.predict(sample, verbose=0, callbacks=[perf_callback], batch_size=batch_size)\n",
    "\n",
    "\t\t\ttelemetry['framework'].append(\"TF (batch_size=N)\")\n",
    "\t\t\ttelemetry['model_name'].append(model_name)\n",
    "\t\t\ttelemetry['rep'].append(i)\n",
    "\t\t\ttelemetry['batch_size'].append(batch_size)\n",
    "\t\ttelemetry['elapsed_time'].extend(perf_callback.latency_ref)\n",
    "\n",
    "\t\t# model.predict_on_batch() =======================================================\n",
    "\t\tfor i in trange(warmup_steps, desc=f\"Warmup for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\t_ = model.predict_on_batch(sample)\n",
    "\n",
    "\t\tperf_callback.latency_ref.clear()\n",
    "\t\tfor i in trange(repetitions, desc=f\"Predict on batch for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\tperf_callback.on_predict_begin()  # tf api is a joke why predict_on_batch has no callbacks i hate it here\n",
    "\t\t\t_ = model.predict_on_batch(sample)\n",
    "\t\t\tperf_callback.on_predict_end()\n",
    "\n",
    "\t\t\ttelemetry['framework'].append(\"TF (predict_on_batch)\")\n",
    "\t\t\ttelemetry['model_name'].append(model_name)\n",
    "\t\t\ttelemetry['rep'].append(i)\n",
    "\t\t\ttelemetry['batch_size'].append(batch_size)\n",
    "\t\ttelemetry['elapsed_time'].extend(perf_callback.latency_ref)\n",
    "\t\t\n",
    "\t\t# model(x) =======================================================================\n",
    "\t\tfor i in trange(warmup_steps, desc=f\"Warmup for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\t_ = model(sample, training=False)\n",
    "\n",
    "\t\tperf_callback.latency_ref.clear()\n",
    "\t\tfor i in trange(repetitions, desc=f\"__call__ for {model_name} (batch of {batch_size})\"):\n",
    "\t\t\tperf_callback.on_predict_begin()\n",
    "\t\t\t_ = model(sample, training=False)\n",
    "\t\t\tperf_callback.on_predict_end()\n",
    "\n",
    "\t\t\ttelemetry['framework'].append(\"TF (__call__)\")\n",
    "\t\t\ttelemetry['model_name'].append(model_name)\n",
    "\t\t\ttelemetry['rep'].append(i)\n",
    "\t\t\ttelemetry['batch_size'].append(batch_size)\n",
    "\t\ttelemetry['elapsed_time'].extend(perf_callback.latency_ref)\n",
    "\n",
    "\t\tdel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(telemetry)\n",
    "\n",
    "if second_run:\n",
    "\tfirst_part = pd.read_csv(f\"../../results_ultimate_0/tensorflow-batch-size-comp-{INDEX}.csv\")\n",
    "\tresults = pd.concat([first_part, results])\n",
    "\n",
    "results.to_csv(f\"../../results_ultimate_0/tensorflow-batch-size-comp-{INDEX}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>framework</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF (batch_size=None)</td>\n",
       "      <td>FullyConnectedNet</td>\n",
       "      <td>1</td>\n",
       "      <td>13692269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF (batch_size=None)</td>\n",
       "      <td>FullyConnectedNet</td>\n",
       "      <td>1</td>\n",
       "      <td>14100517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF (batch_size=None)</td>\n",
       "      <td>FullyConnectedNet</td>\n",
       "      <td>1</td>\n",
       "      <td>13857866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF (batch_size=None)</td>\n",
       "      <td>FullyConnectedNet</td>\n",
       "      <td>1</td>\n",
       "      <td>13737609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TF (batch_size=None)</td>\n",
       "      <td>FullyConnectedNet</td>\n",
       "      <td>1</td>\n",
       "      <td>13772256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              framework         model_name  batch_size  elapsed_time\n",
       "0  TF (batch_size=None)  FullyConnectedNet           1      13692269\n",
       "1  TF (batch_size=None)  FullyConnectedNet           1      14100517\n",
       "2  TF (batch_size=None)  FullyConnectedNet           1      13857866\n",
       "3  TF (batch_size=None)  FullyConnectedNet           1      13737609\n",
       "4  TF (batch_size=None)  FullyConnectedNet           1      13772256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>framework</th>\n",
       "      <th>model_name</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>elapsed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>1</td>\n",
       "      <td>1.047642e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>16</td>\n",
       "      <td>1.029272e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>32</td>\n",
       "      <td>1.039008e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>64</td>\n",
       "      <td>1.039095e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>96</td>\n",
       "      <td>1.189673e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>128</td>\n",
       "      <td>1.045872e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>192</td>\n",
       "      <td>1.044369e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>ConvNeXt-Tiny</td>\n",
       "      <td>256</td>\n",
       "      <td>1.037088e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>1</td>\n",
       "      <td>1.424170e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>16</td>\n",
       "      <td>1.420804e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>32</td>\n",
       "      <td>1.449514e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>64</td>\n",
       "      <td>1.445390e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>96</td>\n",
       "      <td>1.457797e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>128</td>\n",
       "      <td>1.436814e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TF (__call__)</td>\n",
       "      <td>DenseNet-121</td>\n",
       "      <td>192</td>\n",
       "      <td>1.436262e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        framework     model_name  batch_size  elapsed_time\n",
       "0   TF (__call__)  ConvNeXt-Tiny           1  1.047642e+08\n",
       "1   TF (__call__)  ConvNeXt-Tiny          16  1.029272e+08\n",
       "2   TF (__call__)  ConvNeXt-Tiny          32  1.039008e+08\n",
       "3   TF (__call__)  ConvNeXt-Tiny          64  1.039095e+08\n",
       "4   TF (__call__)  ConvNeXt-Tiny          96  1.189673e+08\n",
       "5   TF (__call__)  ConvNeXt-Tiny         128  1.045872e+08\n",
       "6   TF (__call__)  ConvNeXt-Tiny         192  1.044369e+08\n",
       "7   TF (__call__)  ConvNeXt-Tiny         256  1.037088e+08\n",
       "8   TF (__call__)   DenseNet-121           1  1.424170e+08\n",
       "9   TF (__call__)   DenseNet-121          16  1.420804e+08\n",
       "10  TF (__call__)   DenseNet-121          32  1.449514e+08\n",
       "11  TF (__call__)   DenseNet-121          64  1.445390e+08\n",
       "12  TF (__call__)   DenseNet-121          96  1.457797e+08\n",
       "13  TF (__call__)   DenseNet-121         128  1.436814e+08\n",
       "14  TF (__call__)   DenseNet-121         192  1.436262e+08"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.drop([\"rep\"], axis=1)\n",
    "display(results.head())\n",
    "\n",
    "results = results.groupby([\"framework\", \"model_name\", \"batch_size\"])\n",
    "results.mean().head(15).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, ds, _ = env_builder(model_name, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
